---
title: "Lecture 3.1 - Practicing Confidence Intervals"
author: "Student"
date: "11/13/2025"
format:
  pdf:
    colorlinks: true
---

![](Titanic.jpg)

```{r}
#| label: setup
#| include: false

# Add any libraries and load your dataset here

library(tidyverse)
library(kableExtra)
library(gtsummary)
library(broom)

titanic.pax <- read.csv("titanic_survival.csv")

theme_set(theme_classic())

prop.multiple.samples <- function(n, numsamples, variable) { 
     meanvector <- c() 
     meanonesample <- 0 
     for (i in 1:numsamples) { 
       meanonesample <- mean(sample(variable, n, replace=TRUE)) 
         meanvector[i] <- meanonesample 
     } 
     meanvector 
}
```

# Interpretation steps

Let's practice with a dataset of the passengers of the Titanic, from the dataset `titanic_survival.csv`.

## Setting things up

1.  Create new variable called `survivedindicator`; assign it a value of 0 if the passenger did not survive and 1 if the passenger did survive using the `case_when` verb (information on how to do that can be found [here](https://www.dkustats101.org/rmanual.html#recoding-variables) or [here](https://shanghai.hosting.nyu.edu/data/r/data-transformation.html#mutate()_to_add_new_variables_at_the_same_time)) . Find the proportion that survived in the entire dataset (you can do this by using the `table()` command: `table(titanic$survivedindicator)`.

```{r}
#| label: tbl-surviveddist
#| tbl-cap: "Survival percentage of passengers on the Titanic"
#| echo: true

titanic.pax <- titanic.pax %>% 
  mutate(survivedindicator = case_when(survived == "yes" ~ 1,
                                       survived == "no" ~ 0)) 

titanic.pax %>% 
  group_by(survivedindicator) %>%
  summarise(Count = n(),
            Percent = round(100 * n() / nrow(titanic.pax), 1)) %>%
  select(survivedindicator, Count, Percent) %>%
  kable(caption = "Summary of Passenger Survival",
        col.names = c("Survived", "Count", "Percent (%)"),
        align = "lcc") %>%
  kable_styling()
```

## Random matters

Now let's sample from the dataset. We can sample whether or not they survived by running the following code (note: you need the library `dplyr` loaded for this code):

```
titanic_n50 <- titanic_survival %>% 
  slice_sample(n=50) 
```

Create a sample of size 50 and 200.

```{r}
#| label: createsamples
#| echo: true

tp.50 <- titanic.pax %>% 
  slice_sample(n=50)

tp.200 <- titanic.pax %>% 
  slice_sample(n=200)
```

2.  Calculate by hand the standard error and 95% confidence interval for `survivedindicator` of your sample of 50 and 200.

```{r}
#| label: samplescis
#| echo: false

p.50 <- round(mean(tp.50$survivedindicator), 2)
se.50 <- round(sqrt(p.50 * (1 - p.50) / 50), 2)
lower.50 <- p.50 - 1.96 * se.50
upper.50 <- p.50 + 1.96 * se.50

p.200 <- round(mean(tp.200$survivedindicator), 2)
se.200 <- round(sqrt(p.200 * (1 - p.200) / 200), 2)
lower.200 <- p.200 - 1.96 * se.200
upper.200 <- p.200 + 1.96 * se.200
```

> For $n=50$, the calculation is: 

- $`r p.50`\pm z_{95} \sqrt{\frac{\hat{`r p.50`}(1 - \hat{`r p.50`})}{50}}$ 
- $`r p.50`\pm 1.96\times`r se.50`$
- $`r p.50`\pm `r 1.96 * se.50`$
- $(`r lower.50`, `r upper.50`)$

> For $n=200$, the calculation is: 

- $`r p.200`\pm z_{95} \sqrt{\frac{\hat{`r p.200`}(1 - \hat{`r p.200`})}{200}}$ 
- $`r p.200`\pm 1.96\times`r se.200`$
- $`r p.200`\pm `r 1.96 * se.200`$
- $(`r lower.200`, `r upper.200`)$

3.  Interpret these confidence intervals

> We are 95% confident that our confidence interval contains the true proportion. In practical terms, the actual proportion of percentage survived seems likely to be less than half.

4.  Find the proportion that survived of the entire dataset -- was it inside or outside the standard error of your confidence intervals for 50 and 200? Why was it inside or outside?

```{r}
#| label: didcontaintruep
#| echo: false
#| results: asis

true.p <- round(mean(titanic.pax$survivedindicator), 2)
cat(paste0("The true p is: ", true.p, "\n\n"))

if(true.p >= lower.50 & true.p <= upper.50) {
  cat("Confidence interval for the sample of 50 DID contain the true p\n\n")
} else {
  cat("Confidence interval for the sample of 50 DID NOT contain the true p\n\n")
}

if(true.p >= lower.200 & true.p <= upper.200) {
  cat("Confidence interval for the sample of 200 DID contain the true p\n\n")
} else {
  cat("Confidence interval for the sample of 200 DID NOT contain the true p\n\n")
}
```

5.  If you sampled many times, how many sample proportions would be inside or outside of your confidence interval you just created? 

> By definition, 95%.

# Sampling distributions

Let's now add the following command to the `setup` block (copy and paste the entire part and then run your `setup` code block again:

```         
prop.multiple.samples <- function(n, numsamples, variable) { 
     meanvector <- c() 
     meanonesample <- 0 
     for (i in 1:numsamples) { 
       meanonesample <- mean(sample(variable, n, replace=TRUE)) 
         meanvector[i] <- meanonesample 
     } 
     meanvector 
}
```

This defines a new function in R called `prop.multiple.samples()`. It takes as its arguments the sample size (`n`), the number of samples (`numsamples`) and the variable from which you would like to create a sampling distribution. Once you have created this function, you can use it as follows:

```         
titanic_n50_s100 <-prop.multiple.samples(50, 100, titanic_survival$survivedindicator)
```
This takes 100 samples of $n=50$ from the variable `titanic_survival$survivedindicator`. In practical terms, this line of code draws 50 people at random from the dataset 100 times and then calculate the proportion in each sample of the number of people surviving. 

This process creates a pseudo sampling distribution.

## Observing the sampling distribution

6.  Make a histogram using `ggplot` of the results of `titanic_n50_s100`. What does this histogram show? Interpret this carefully. 

```{r}
#| label: fig-histn50s100
#| fig-cap: "Histogram of Titanice passenger surived percentage"
#| fig-subcap: "N=50, Nsample=100"
#| echo: false
#| message: false

titanic.n50.s100 <-prop.multiple.samples(50, 100, titanic.pax$survivedindicator)

ggplot(data.frame(titanic.n50.s100), aes(x=titanic.n50.s100)) + 
  geom_histogram(color="black", fill="blue", bins=10) +
  labs(x="Sample survived percent", y="Count")
```


7.  Calculate the `sd()` of `titanic_n50_s100` - what is this quantity indicate? What calculation should it be equal to? Why?

```{r}
#| label: sdn50s100
#| echo: true

sd(titanic.n50.s100)
```

> This should be equal to the calculated SE.

8.  If you increased the number of items sampled (`n` or the first entry in `prop.multiple.samples`) what do you think will happen to your histogram? How about the `sd()` you calculated?. How about if you increased `numsamples` instead?

> The size of the standard deviation of the sampling distribution should decrase as the sample size increases (sample to sample variation should decrease). Therefore, the spread indicated by the histogram should also decrease. 

> If numsamples increases, the stability of the shape of the sampling distribution should improve - it should become more obviously normal.

## Comparing the distribution

9.  Increase the `n` and `numsamples` separately (try values like 200 and 500 and 10000). How does the shape and distribution of the histogram change? Did it match your expectations? Why or why not?

```{r}
#| label: fig-increasen
#| fig-cap: "Channging the value of sample size (N)"
#| fig-subcap: 
#|    - "N=200, Nsample=100"
#|    - "N=500, Nsample=100"
#|    - "N=1000, Nsample=100"
#| layout-ncol: 2
#| echo: false

titanic.n200.s100 <-prop.multiple.samples(200, 100, titanic.pax$survivedindicator)
titanic.n500.s100 <-prop.multiple.samples(500, 100, titanic.pax$survivedindicator)
titanic.n1000.s100 <-prop.multiple.samples(1000, 100, titanic.pax$survivedindicator)

ggplot(data.frame(titanic.n200.s100), aes(x=titanic.n200.s100)) + 
  geom_histogram(color="black", fill="cyan", bins=10) +
  labs(x="Sample survived percent", y="Count")

ggplot(data.frame(titanic.n500.s100), aes(x=titanic.n500.s100)) + 
  geom_histogram(color="black", fill="magenta", bins=10) +
  labs(x="Sample survived percent", y="Count")

ggplot(data.frame(titanic.n1000.s100), aes(x=titanic.n1000.s100)) + 
  geom_histogram(color="black", fill="yellow", bins=10) +
  labs(x="Sample survived percent", y="Count")
```

> As expected, the distribution of the sampling distribution decreased as the sample size increased.

```{r}
#| label: fig-increasensanmple
#| fig-cap: "Channging the value of sample size (N)"
#| fig-subcap: 
#|    - "N=50, Nsample=200"
#|    - "N=50, Nsample=500"
#|    - "N=50, Nsample=1000"
#| layout-ncol: 2
#| echo: false

titanic.n50.s200 <-prop.multiple.samples(50, 200, titanic.pax$survivedindicator)
titanic.n50.s500 <-prop.multiple.samples(50, 500, titanic.pax$survivedindicator)
titanic.n50.s1000 <-prop.multiple.samples(50, 1000, titanic.pax$survivedindicator)

ggplot(data.frame(titanic.n50.s200), aes(x=titanic.n50.s200)) + 
  geom_histogram(color="black", fill="red", bins=10) +
  labs(x="Sample survived percent", y="Count")

ggplot(data.frame(titanic.n50.s500), aes(x=titanic.n50.s500)) + 
  geom_histogram(color="black", fill="blue", bins=15) +
  labs(x="Sample survived percent", y="Count")

ggplot(data.frame(titanic.n50.s1000), aes(x=titanic.n50.s1000)) + 
  geom_histogram(color="black", fill="green", bins=20) +
  labs(x="Sample survived percent", y="Count")
```

> Here the difference is less obvious but the shape does appear to be more normally distributed.

# Extra activity - modeling

While it is more common to use a non-linear link function (logistic regression) to model an outcome variable with a 0 or 1 outcome, in this case please try to use linear regression make a model that predicts what factors are most important in predicting survival on the Titanic.

```{r}
#| label: tbl-logregtbl
#| tbl-cap: "Logistic regression model predicting status: survived"
#| echo: false

# Fit logistic regression
logit_fit <- glm(survivedindicator ~ passengerclass + sex + age, data = titanic.pax, 
                 family = binomial)

# Compute model statistics
gl <- glance(logit_fit)
r2_mcfadden <- 1 - (gl$deviance / gl$null.deviance)
resid_se <- sqrt(gl$deviance / gl$df.residual)

# Create gtsummary table
tbl <- tbl_regression(logit_fit, exponentiate = TRUE, intercept = TRUE) %>%
  modify_source_note(paste0("McFadden R-squared: ", round(r2_mcfadden, 3),
                            "; Residual Std. Error (âˆšdeviance/df): ", round(resid_se, 3)))

tbl
```

