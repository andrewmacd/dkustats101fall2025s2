[
  {
    "objectID": "schedulematerials.html",
    "href": "schedulematerials.html",
    "title": "Course Schedule and Class Materials",
    "section": "",
    "text": "Important: class schedule is subject to change, contingent on mitigating circumstances and the progress we make as a class. If there are any changes, I will announce them on Teams."
  },
  {
    "objectID": "schedulematerials.html#lecture-1.1-class-welcome-thursday-october-23",
    "href": "schedulematerials.html#lecture-1.1-class-welcome-thursday-october-23",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 1.1: Class welcome (Thursday, October 23)",
    "text": "Lecture 1.1: Class welcome (Thursday, October 23)\nReading to do before class: Chapter 1, 2.1 and 2.2, and 3\nTopics covered:\n\nWhat are data and variables?\nHow to display quantitative and qualitative variables\nContingency tables"
  },
  {
    "objectID": "schedulematerials.html#lab-1.1-r-and-quarto-familiarization-thursday-october-23",
    "href": "schedulematerials.html#lab-1.1-r-and-quarto-familiarization-thursday-october-23",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 1.1: R and Quarto familiarization (Thursday, October 23)",
    "text": "Lab 1.1: R and Quarto familiarization (Thursday, October 23)\n\nLab files: Lab 1.1"
  },
  {
    "objectID": "schedulematerials.html#online-lab-1.1-due-sunday-october-26-at-115900-pm",
    "href": "schedulematerials.html#online-lab-1.1-due-sunday-october-26-at-115900-pm",
    "title": "Course Schedule and Class Materials",
    "section": "Online lab 1.1 (due Sunday, October 26 at 11:59:00 pm)",
    "text": "Online lab 1.1 (due Sunday, October 26 at 11:59:00 pm)\n\nIntroduction to R\nIntroduction to the Tidyverse"
  },
  {
    "objectID": "schedulematerials.html#lecture-1.2-characteristics-of-distributions-tuesday-october-28",
    "href": "schedulematerials.html#lecture-1.2-characteristics-of-distributions-tuesday-october-28",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 1.2: Characteristics of distributions (Tuesday, October 28)",
    "text": "Lecture 1.2: Characteristics of distributions (Tuesday, October 28)\nReading to do before class: Chapter 2.3-5 and 4\nTopics covered:\n\nHow to describe the shape, center, and spread of a distribution\nHow to compare distributions\nDealing with problem distributions (outliers, reexpression)\nLecture webpage: Lecture 1.2\nLecture activity: Lecture 1.2 activity"
  },
  {
    "objectID": "schedulematerials.html#lecture-1.3-comparing-distributions-and-the-normal-distribution-thursday-october-30",
    "href": "schedulematerials.html#lecture-1.3-comparing-distributions-and-the-normal-distribution-thursday-october-30",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 1.3: Comparing distributions and the Normal distribution (Thursday, October 30)",
    "text": "Lecture 1.3: Comparing distributions and the Normal distribution (Thursday, October 30)\nReading to do before class: Chapter 5\nTopics covered:\n\nStandard deviation and standardizing values\nNormal models\nNormal percentiles\nLecture webpage: Lecture 1.3\nLecture activity: Lecture 1.3 activity"
  },
  {
    "objectID": "schedulematerials.html#lab-1.2-advanced-quarto-editing-thursday-october-30",
    "href": "schedulematerials.html#lab-1.2-advanced-quarto-editing-thursday-october-30",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 1.2: Advanced Quarto editing (Thursday, October 30)",
    "text": "Lab 1.2: Advanced Quarto editing (Thursday, October 30)\n\nLab files: Lab 1.2\n\nMake sure to extract (unzip) the lab files before attempting to modify them!"
  },
  {
    "objectID": "schedulematerials.html#unit-1-homework---progress-check-due-thursday-october-30-at-235900",
    "href": "schedulematerials.html#unit-1-homework---progress-check-due-thursday-october-30-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Unit 1 homework - progress check (due Thursday, October 30 at 23:59:00)",
    "text": "Unit 1 homework - progress check (due Thursday, October 30 at 23:59:00)\n\nHomework files: Unit 1 homework instructions"
  },
  {
    "objectID": "schedulematerials.html#online-lab-1.2-due-on-friday-october-31-at-235900",
    "href": "schedulematerials.html#online-lab-1.2-due-on-friday-october-31-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Online lab 1.2 (due on Friday, October 31 at 23:59:00)",
    "text": "Online lab 1.2 (due on Friday, October 31 at 23:59:00)\n\nIntroduction to Data Visualization with ggplot2"
  },
  {
    "objectID": "schedulematerials.html#unit-1-homework-due-sunday-november-2-at-235900-pm",
    "href": "schedulematerials.html#unit-1-homework-due-sunday-november-2-at-235900-pm",
    "title": "Course Schedule and Class Materials",
    "section": "Unit 1 homework (due Sunday, November 2 at 23:59:00 pm)",
    "text": "Unit 1 homework (due Sunday, November 2 at 23:59:00 pm)\n\nHomework files: Unit 1 homework\nHomework sample solutions: [Unit 1 homework sample solutions]"
  },
  {
    "objectID": "schedulematerials.html#lecture-2.1-association-and-correlation-tuesday-november-4",
    "href": "schedulematerials.html#lecture-2.1-association-and-correlation-tuesday-november-4",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 2.1: Association and correlation (Tuesday, November 4)",
    "text": "Lecture 2.1: Association and correlation (Tuesday, November 4)\nReading to do before class: Chapter 6\nTopics covered:\n\nScatterplots\nCorrelations\nDoes correlation imply causation?\nLecture webpage: Lecture 2.1\nLecture activity: Lecture 2.1 activity\n\nSample solution: Lecture 2.1 activity sample solution"
  },
  {
    "objectID": "schedulematerials.html#lecture-2.2-simple-linear-regression-thursday-november-6",
    "href": "schedulematerials.html#lecture-2.2-simple-linear-regression-thursday-november-6",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 2.2: Simple Linear Regression (Thursday, November 6)",
    "text": "Lecture 2.2: Simple Linear Regression (Thursday, November 6)\nReading to do before class: Chapter 7\nTopics covered:\n\nLine of best fit: least squares\nThe linear model\nWhat are residuals\nRegression assumptions\nLecture webpage: Lecture 2.2\nLecture activity: Lecture 2.2 activity"
  },
  {
    "objectID": "schedulematerials.html#lab-2.1-working-with-regressions-using-dplyr-thursday-november-6",
    "href": "schedulematerials.html#lab-2.1-working-with-regressions-using-dplyr-thursday-november-6",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 2.1: Working with regressions using dplyr (Thursday, November 6)",
    "text": "Lab 2.1: Working with regressions using dplyr (Thursday, November 6)\n\nLab files: Lab 2.1\n\nMake sure to extract (unzip) the lab files before attempting to modify them!\nSample solution:Lab 2.1 Sample Solution"
  },
  {
    "objectID": "schedulematerials.html#lecture-2.3-regression-wisdom-friday-november-7",
    "href": "schedulematerials.html#lecture-2.3-regression-wisdom-friday-november-7",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 2.3: Regression Wisdom (Friday, November 7)",
    "text": "Lecture 2.3: Regression Wisdom (Friday, November 7)\n\nNote the special class time!\n\nReading to do before class: Chapter 8\nTopics covered:\n\nBeware extrapolation\nOutliers and leverage\nLurking variables\nStraightening scatterplots\nLecture webpage: [Lecture 2.3]\nLecture activity: [Lecture 2.3 activity]"
  },
  {
    "objectID": "schedulematerials.html#online-lab-2.2-due-on-friday-november-7-at-235900",
    "href": "schedulematerials.html#online-lab-2.2-due-on-friday-november-7-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Online lab 2.2 (due on Friday, November 7 at 23:59:00)",
    "text": "Online lab 2.2 (due on Friday, November 7 at 23:59:00)\n\nIntermediate Data Visualization with ggplot2"
  },
  {
    "objectID": "schedulematerials.html#unit-2-homework---progress-check-due-sunday-november-9-at-235900",
    "href": "schedulematerials.html#unit-2-homework---progress-check-due-sunday-november-9-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Unit 2 homework - progress check (due Sunday, November 9 at 23:59:00)",
    "text": "Unit 2 homework - progress check (due Sunday, November 9 at 23:59:00)\n\nHomework files: Unit 2 homework instructions"
  },
  {
    "objectID": "schedulematerials.html#lecture-2.4-multiple-regression-tuesday-november-11",
    "href": "schedulematerials.html#lecture-2.4-multiple-regression-tuesday-november-11",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 2.4: Multiple Regression (Tuesday, November 11)",
    "text": "Lecture 2.4: Multiple Regression (Tuesday, November 11)\nReading to do before class: Chapter 9\nTopics covered:\n\nWhat is multiple regression?\nInterpreting multiple regression coefficients\nPartial regression plots\nIndicator variables\nLecture webpage: [Lecture 2.4]\nLecture activity: [Lecture 2.4 activity]"
  },
  {
    "objectID": "schedulematerials.html#unit-2-homework-due-sunday-november-16-at-115900",
    "href": "schedulematerials.html#unit-2-homework-due-sunday-november-16-at-115900",
    "title": "Course Schedule and Class Materials",
    "section": "Unit 2 homework (due Sunday, November 16 at 11:59:00)",
    "text": "Unit 2 homework (due Sunday, November 16 at 11:59:00)\n\nHomework files: Unit 2 homework instructions"
  },
  {
    "objectID": "schedulematerials.html#lecture-3.1-confidence-intervals---proportions-thursday-november-13",
    "href": "schedulematerials.html#lecture-3.1-confidence-intervals---proportions-thursday-november-13",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 3.1: Confidence intervals - proportions (Thursday, November 13)",
    "text": "Lecture 3.1: Confidence intervals - proportions (Thursday, November 13)\nReading to do before class: Chapter 13\nTopics covered:\n\nWhat is a sampling distribution?\nWhen does the normal model apply?\nConstructing a confidence interval\nInterpreting a confidence interval\nLecture webpage: [Lecture 3.1]\nLecture activity: [Lecture 3.1 activity]"
  },
  {
    "objectID": "schedulematerials.html#lab-2.2-interpreting-coefficients-thursday-november-13",
    "href": "schedulematerials.html#lab-2.2-interpreting-coefficients-thursday-november-13",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 2.2: Interpreting coefficients (Thursday, November 13)",
    "text": "Lab 2.2: Interpreting coefficients (Thursday, November 13)\n\nLab files: [Lab 2.2]\n\nMake sure to extract (unzip) the lab files before attempting to modify them!"
  },
  {
    "objectID": "schedulematerials.html#lecture-3.2-confidence-intervals---means-tuesday-november-18",
    "href": "schedulematerials.html#lecture-3.2-confidence-intervals---means-tuesday-november-18",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 3.2: Confidence intervals - means (Tuesday, November 18)",
    "text": "Lecture 3.2: Confidence intervals - means (Tuesday, November 18)\nReading to do before class: Chapter 14\nTopics covered:\n\nThe Central Limit Theorem\nConfidence interval for means\nInterpreting a confidence interval\nFinal thoughts on confidence intervals\nLecture webpage: [Lecture 3.2]\nNo lecture activity"
  },
  {
    "objectID": "schedulematerials.html#lecture-3.3-hypothesis-testing-thursday-november-20",
    "href": "schedulematerials.html#lecture-3.3-hypothesis-testing-thursday-november-20",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 3.3: Hypothesis testing (Thursday, November 20)",
    "text": "Lecture 3.3: Hypothesis testing (Thursday, November 20)\nReading to do before class: Chapter 15\nTopics covered:\n\nWhat are hypotheses?\n\\(p\\) values\n\\(p\\) values and decisions – how to make a decision\nLecture webpage: [Lecture 3.3]\nLecture activity: [Lecture 3.3 activity]"
  },
  {
    "objectID": "schedulematerials.html#lab-3.1-bootstrapping-thursday-november-20",
    "href": "schedulematerials.html#lab-3.1-bootstrapping-thursday-november-20",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 3.1: Bootstrapping (Thursday, November 20)",
    "text": "Lab 3.1: Bootstrapping (Thursday, November 20)\n\nLab files: [Lab 3.1]\n\nMake sure to extract (unzip) the lab files before attempting to modify them!"
  },
  {
    "objectID": "schedulematerials.html#lecture-3.4-hypothesis-testing-wisdom-tuesday-november-25",
    "href": "schedulematerials.html#lecture-3.4-hypothesis-testing-wisdom-tuesday-november-25",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 3.4: Hypothesis testing wisdom (Tuesday, November 25)",
    "text": "Lecture 3.4: Hypothesis testing wisdom (Tuesday, November 25)\nReading to do before class: Chapter 16\nTopics covered:\n\nInterpreting p-values\nAlpha and critical values\nPractical vs. statistical significance\nType I and II errors\nPower of a test\nLecture webpage: [Lecture 3.4]\nLecture activity: [Lecture 3.4 activity]"
  },
  {
    "objectID": "schedulematerials.html#in-class-unit-3-exam-thursday-november-27-from-600-pm-to-715-pm",
    "href": "schedulematerials.html#in-class-unit-3-exam-thursday-november-27-from-600-pm-to-715-pm",
    "title": "Course Schedule and Class Materials",
    "section": "In-class Unit 3 exam: Thursday, November 27 from 6:00 pm to 7:15 pm",
    "text": "In-class Unit 3 exam: Thursday, November 27 from 6:00 pm to 7:15 pm"
  },
  {
    "objectID": "schedulematerials.html#lecture-4.1-comparing-groups-thursday-november-27",
    "href": "schedulematerials.html#lecture-4.1-comparing-groups-thursday-november-27",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 4.1: Comparing groups (Thursday, November 27)",
    "text": "Lecture 4.1: Comparing groups (Thursday, November 27)\nReading to do before class: Chapter 17\nTopics covered:\n\nConfidence intervals for comparing two samples\nAssumptions and conditions for two-sample hypothesis tests\nTwo-sample \\(z\\) test\nTwo-sample \\(t\\) test\nLecture webpage: [Lecture 4.1]\nLecture activity: [Lecture 4.1 activity]"
  },
  {
    "objectID": "schedulematerials.html#lecture-4.2-returning-to-regression-tuesday-december-2",
    "href": "schedulematerials.html#lecture-4.2-returning-to-regression-tuesday-december-2",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 4.2: Returning to regression (Tuesday, December 2)",
    "text": "Lecture 4.2: Returning to regression (Tuesday, December 2)\nReading to do before class: Chapter 20\nTopics covered:\n\nRegression inference and intuition\nThe regression table\nConfidence and prediction intervals\nLecture webpage: [Lecture 4.2]\nLecture activity: [Lecture 4.2 activity]"
  },
  {
    "objectID": "schedulematerials.html#final-class-4.3-interpretation-activity-thursday-december-4",
    "href": "schedulematerials.html#final-class-4.3-interpretation-activity-thursday-december-4",
    "title": "Course Schedule and Class Materials",
    "section": "Final class 4.3: Interpretation activity (Thursday, December 4)",
    "text": "Final class 4.3: Interpretation activity (Thursday, December 4)\n\nHow to read academic statistical results\nLocating the model\nInterpreting the test\nDetermining possible weaknesses of the model\nFinal class activity: [Final class 4.3 activity]"
  },
  {
    "objectID": "schedulematerials.html#online-lab-4.1-due-on-friday-december-6-at-115900",
    "href": "schedulematerials.html#online-lab-4.1-due-on-friday-december-6-at-115900",
    "title": "Course Schedule and Class Materials",
    "section": "Online lab 4.1 (due on Friday, December 6 at 11:59:00)",
    "text": "Online lab 4.1 (due on Friday, December 6 at 11:59:00)\n\nYour choice of any DataCamp course (as long as it relates to statistics)\n\nSend the completion certificate from the end of the course to the lab manager, Jingyu Wang, on Teams"
  },
  {
    "objectID": "schedulematerials.html#final-project---progress-check-due-sunday-december-8-at-235900",
    "href": "schedulematerials.html#final-project---progress-check-due-sunday-december-8-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Final project - progress check (due Sunday, December 8 at 23:59:00)",
    "text": "Final project - progress check (due Sunday, December 8 at 23:59:00)\n\nHomework files: [Final project instructions]"
  },
  {
    "objectID": "schedulematerials.html#final-project-due-wednesday-december-10-at-235900",
    "href": "schedulematerials.html#final-project-due-wednesday-december-10-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Final project (due Wednesday, December 10 at 23:59:00)",
    "text": "Final project (due Wednesday, December 10 at 23:59:00)\n\nHomework files: [Final project instructions]"
  },
  {
    "objectID": "lectures/lecture.2.2.html#line-of-best-fit",
    "href": "lectures/lecture.2.2.html#line-of-best-fit",
    "title": "Simple Linear Regression",
    "section": "Line of best fit",
    "text": "Line of best fit\n\n\n\nHDI definition\n\n\nHow to describe the relationship between GDP per capita and HDI score?\nAs we learned:\n\nDirection\nForm\nStrength\nOutlier\n\nWhat do you expect the relationship between GDP per capita and HDI will be?"
  },
  {
    "objectID": "lectures/lecture.2.2.html#scatterplot-of-gdp-per-capita-and-hdi",
    "href": "lectures/lecture.2.2.html#scatterplot-of-gdp-per-capita-and-hdi",
    "title": "Simple Linear Regression",
    "section": "Scatterplot of GDP per capita and HDI",
    "text": "Scatterplot of GDP per capita and HDI\n\n\n\n\n\n\n\n\n\nSmoother\n\n\n\n\n\n\n\n\n\n\nTaking a guess\nWhat do you think the intercept and the slope should be for a line of ‘good’ fit?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast squares line\n\nSlope: 0.4498406, intercept: 0.6025812"
  },
  {
    "objectID": "lectures/lecture.2.2.html#linear-model-1",
    "href": "lectures/lecture.2.2.html#linear-model-1",
    "title": "Simple Linear Regression",
    "section": "Linear model",
    "text": "Linear model\nIt’s better if we come up with a more formal model: \\(\\hat{y}= b_0 + b_1x\\)\n\\(\\hat{y}\\) is our predicted value \\(b_0\\) is the \\(y\\) intercept - the value when \\(x\\) is 0 \\(b_1\\) is the slope\n\nHelps with predictions\n\nFor values not in the sample, we can estimate their HDI score\n\nHelps assess model fit - we can compare different lines more easily\n\nMore specifically we can calculate the residuals\nResiduals are difference between our line and the actually observed value - how much our line ‘missed’ by\n\n\n\nLinear model for our data\n\\(\\hat{y}= 0.6025812 + 0.4498406x\\)"
  },
  {
    "objectID": "lectures/lecture.2.2.html#least-squares-line-1",
    "href": "lectures/lecture.2.2.html#least-squares-line-1",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nBut how to calculate?\nMany different ways\n\nMake a line minimizing the least absolute deviations\nNon-parametric lines\nMake a line minimizing the sum of the squares of the deviations\n\nLeast squares line is most common\n\nAdvantages:\n\nEasy to calculate\nWell understood statistical properties\n\nDisadvantages:\n\nLine will be strongly influenced by outliers"
  },
  {
    "objectID": "lectures/lecture.2.2.html#examining-model-fit",
    "href": "lectures/lecture.2.2.html#examining-model-fit",
    "title": "Simple Linear Regression",
    "section": "Examining model fit",
    "text": "Examining model fit\n\nChecking the residuals\nResidual standard deviation\n\\(R^2\\)\nChecking assumptions\n\n\nChecking the residuals\nAll real datasets have noise so the real formula is:\n\\(y = b_0 + b_1x + e\\)\nResidual = Observed - Predicted\n\n\\(e = y - \\hat{y}\\)\n\nCan easily plot the residuals, put the “size of the miss” on the \\(y\\) axis, and original data on the \\(x\\) axis\n\n\nResiduals - our data"
  },
  {
    "objectID": "lectures/lecture.2.2.html#graphing-the-residuals",
    "href": "lectures/lecture.2.2.html#graphing-the-residuals",
    "title": "Simple Linear Regression",
    "section": "Graphing the residuals",
    "text": "Graphing the residuals\n\n\n\n\n\n\n\n\n\nResiduals vs. observed data"
  },
  {
    "objectID": "lectures/lecture.2.2.html#residual-standard-deviation",
    "href": "lectures/lecture.2.2.html#residual-standard-deviation",
    "title": "Simple Linear Regression",
    "section": "Residual standard deviation",
    "text": "Residual standard deviation\n\nSince the residuals are just another distribution, we can also examine their distribution\n\nWhat to look for: symmetrical, no skew/outliers\nStandard deviation not too large\n\n\n\nResidual standard deviation - our data\n\n\n\n\n\n\n\n\nHow would you interpret this histogram of the residuals?"
  },
  {
    "objectID": "lectures/lecture.2.2.html#r2",
    "href": "lectures/lecture.2.2.html#r2",
    "title": "Simple Linear Regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\\(R^2\\) is just the return of \\(r\\), the correlation coefficient. Remember:\n\n\\(r\\) measures the strength of the association between \\(x\\) and \\(y\\)\n\nThat is, how reliably \\(x\\) varies with \\(y\\)\n\nThe correlation coefficient: 0.78\nOur \\(R^2\\): 0.61\n\n\n\n\n\n\n\n\n\nWhat do you think the \\(R^2\\) will change to when we remove the outlier?\n\n\n\n\n\n\n\n\n\nThe correlation coefficient for a model with the outlier removed:\n\n\n\n\n\n\n\n\n\n\nOur \\(R^2\\) with the outlier removed:"
  },
  {
    "objectID": "lectures/lecture.2.2.html#how-to-interpret-r2",
    "href": "lectures/lecture.2.2.html#how-to-interpret-r2",
    "title": "Simple Linear Regression",
    "section": "How to interpret \\(R^2\\)",
    "text": "How to interpret \\(R^2\\)\n\nIf there are no serious outliers and the relationship is linear, can provide a useful measure of how strongly the predictor variable is related to the response variable\n\nThe two assumptions above are quite strong - you need to always draw a picture to make sure they are true!\nShould not be interpreted as how strongly \\(x\\) causes \\(y\\), we only know about association."
  },
  {
    "objectID": "lectures/lecture.2.2.html#regression-assumptions",
    "href": "lectures/lecture.2.2.html#regression-assumptions",
    "title": "Simple Linear Regression",
    "section": "Regression assumptions",
    "text": "Regression assumptions\n\nQuantitative variable assumption\nStraight enough condition\nOutlier condition\nDoes the plot thicken condition?\n\nHave we met these?"
  },
  {
    "objectID": "lectures/lecture.2.2.html#reexpressions",
    "href": "lectures/lecture.2.2.html#reexpressions",
    "title": "Simple Linear Regression",
    "section": "Reexpressions",
    "text": "Reexpressions\n\nLog reexpressed\n\n\n\n\n\n\n\n\nWhat will happen to the shape of the graph?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog reexpressed - outlier\nAny guess as to the outlier?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutlier\n\n\n\n\n\n\n\n\n\n\nGraphing the residuals - log\n\n\n\n\n\n\n\n\n\n\nResiduals standard deviation - log\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\nThe correlation coefficient: 0.95\nOur \\(R^2\\): 0.91\n\n\n\nRegression assumptions\nFor the log reexpressed version, have the assumptions been met?\n\nQuantitative variable assumption\nStraight enough condition\nOutlier condition\nDoes the plot thicken condition?"
  },
  {
    "objectID": "lectures/lecture.1.3.html#thoughts-about-comparing-groups",
    "href": "lectures/lecture.1.3.html#thoughts-about-comparing-groups",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Thoughts about comparing groups",
    "text": "Thoughts about comparing groups\n\nFaceted histograms are a reasonable display to show distributions by a categorical variable\n\nHowever these displays become hard to interpret when the number of levels in a category grows large\n\nMuch easier to interpret is side by side box plots\nBox plots capture many important characteristics of a distribution into a summary display\nThink carefully about how you treat outliers\nLet’s view data from the 2024-2025 NBA season"
  },
  {
    "objectID": "lectures/lecture.1.3.html#two-group-comparison",
    "href": "lectures/lecture.1.3.html#two-group-comparison",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Two group comparison",
    "text": "Two group comparison\n\nNBA side-by-side histograms of points scored by W/L\n\n\n\n\n\n\n\n\n\n\nNBA boxplot comparison of points scored by W/L\n\n\n\n\n\n\n\n\n\n\nNBA boxplot comparison of points scored by W/L (better)"
  },
  {
    "objectID": "lectures/lecture.1.3.html#many-group-comparison",
    "href": "lectures/lecture.1.3.html#many-group-comparison",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Many group comparison",
    "text": "Many group comparison\n\nNBA side-by-side histograms of points scored by team\n\n\n\n\n\n\n\n\n\n\nNBA boxplot comparison of points scored by team (better)\n\n\n\n\n\n\n\n\n\n\nYour turn\n\nWork with your neighbor to analyze a different set of statistics\n\nCan be by division or not\nRemember the key features of distributions\n\nShape\nCenter\nSpread\n\n\nInterpret your results\nVariable definitions:\n\nMP: minutes played\nFG: field goal baskets made\nFGA: field goal baskets attempted\nFG.: field goal percentage\nX3P: three pointers made\nX3PA: three pointers attempted\nFT: free throws made\nFTA: free throws attempted\nFT.: free throw percentage\nORB: offensive rebounds\nDRB: defensive rebounds\nTRB: total rebounds\nAST: assists\nSTL: steals\nBLK: blocks\nTOV: turnovers\nPF: personal fouls\nPTS: points scored"
  },
  {
    "objectID": "lectures/lecture.1.3.html#checking-outliers---assists",
    "href": "lectures/lecture.1.3.html#checking-outliers---assists",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking outliers - assists",
    "text": "Checking outliers - assists\n\nOutliers - assists\n\n\n\n\n\n\n\n\n\n\nAssists &gt; 40 - true outliers?"
  },
  {
    "objectID": "lectures/lecture.1.3.html#checking-outliers---points",
    "href": "lectures/lecture.1.3.html#checking-outliers---points",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking outliers - points",
    "text": "Checking outliers - points\n\nOutliers - points\n\n\n\n\n\n\n\n\n\n\nPoints by team &gt; 150 - true outliers?"
  },
  {
    "objectID": "lectures/lecture.1.3.html#in-summary",
    "href": "lectures/lecture.1.3.html#in-summary",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "In summary",
    "text": "In summary\n\nThink about which kind of display is appropriate for comparing distributions\nWhen conditioning on a categorical variable, boxplots are usually better\nBut boxplots lose information\nThink carefully about omitting outliers\nOutliers may reveal important information about your dataset!"
  },
  {
    "objectID": "lectures/lecture.1.3.html#titanic-passengers-and-the-normal-distribution",
    "href": "lectures/lecture.1.3.html#titanic-passengers-and-the-normal-distribution",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Titanic passengers and the Normal distribution",
    "text": "Titanic passengers and the Normal distribution\n\n\n\nTitanic\n\n\n\nDataset of passengers on the Titanic\n\n\n\n\n\n\n\n\n\nWhat are your expectations for how age should be distributed?\n\n\n\n\n\n\n\n\n\n\nWe are going to violate our first three rules:\n\nMake a picture\nMake a picture\nMake a picture\n\n\n\n\nWere the passenger ages normally distributed?\nTo answer that question, we need some information about the distribution\nRemember, our main information about distributions is:\n\nShape\nCenter\nSpread\n\n\n\nInformation about age\n\nStandard deviation: 14.4\nMean: 29.9\nNormal model: \\(N(\\mu, \\sigma) = N(29.9,14.4)\\)\n\n\\(\\mu\\) is the theoretical mean\n\\(\\sigma\\) is the theoretical standard deviation\nThese values define the data generating process\nWe only see some values of the data generating process, but if we saw infinite values, the mean would be \\(\\mu\\) and the sd would be \\(\\sigma\\)\nMore on this in the second half of class\n\nHow can we check normality using this information?"
  },
  {
    "objectID": "lectures/lecture.1.3.html#checking-normality",
    "href": "lectures/lecture.1.3.html#checking-normality",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking normality",
    "text": "Checking normality\n\nThinking about normality\n\nWe can check normality by comparing the quantiles of our data with that of the known quantiles of the normal distribution\n\nWe know approximately 95% of the data lies within two standard deviations\nTherefore, 2.5% data with the lowest values lie outside of -2 standard deviations and 2.5% of data with the highest values lie outside of 2 standard deviations\n\nSimilarly, we know the same information for data within one standard deviation (16%, 68%, 16%)\n\n\n\nData within standard deviations"
  },
  {
    "objectID": "lectures/lecture.1.3.html#checking-against-the-data",
    "href": "lectures/lecture.1.3.html#checking-against-the-data",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking against the data",
    "text": "Checking against the data\n\nHistogram of ages from the data\n\n\n\n\n\n\n\n\n\n\nNormality and scaling\n\nNote that normality does not depend on the size of the standard deviation or the size of the mean\nCould easily change the units to be months instead of years\n\nMean would increase a lot\nStandard deviation would increase a lot\nHowever, amount of observations within each standard deviation would stay the same"
  },
  {
    "objectID": "lectures/lecture.1.3.html#final-thoughts-on-normality",
    "href": "lectures/lecture.1.3.html#final-thoughts-on-normality",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Final thoughts on normality",
    "text": "Final thoughts on normality\n\nWhen is the normal distribution useful?\n\nWhen we know a data-generating process is normally distributed we don’t even need to sample the population\n\nCan find out exactly how much data is between a certain number of standard deviations\n\nWhen we expect a data-generating process to be normally distributed, can test for deviations from normality\n\nIn the case of Titanic passengers, some parts of the distribution were more bunched up, others more spread out\n\nA lot of our statistical techniques require or work better when the data is ‘roughly’ normal\n\nWill detail these in the coming weeks\n\nWe can transform our data to be closer to normal\n\nNote that transformations won’t work if the data has multiple modes, can only correct skew\n\n\n\n\nWhat transformation would be helpful for age?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Tuesday/Thursday class at 12:00 pm to 2:30 pm in Library room 2001\nThursday lab at 6:00 pm to 7:15 pm in Library room 2001\nOffice hours:\n\nWednesday from 1:00 pm to 5:00 pm in WDR 3114\n\nHomeworks are all due at 11:59 pm\nAll other announcements and information are posted on the class Teams site\nStats 101 lab manager: Jingyu Wang\n\nOffice hours: Fridays from 2:00 to 4:00 pm in WDR 2134"
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Course Introduction",
    "section": "Assessment",
    "text": "Assessment\n\nHomework 65%: At the end of the first two groups of content, a homework will be assigned that will ask you to analyze a dataset and answer questions related to that concept group. These homeworks are due at the noted date at 11:59 pm China time. Each homework will also have a best graph contest, where the person whose classmates vote as having the best graph wins some extra credit points.\n\nHomework 1: 10%\nHomework 2: 25%\nFinal project: 30%\n\nHomework checks 3%: To make sure you are making good progress on your homework, the Sunday before the homework is due (except for the first homework) you will be required to submit your progress on the homework so far. You are required to have tried to have answered all the questions covered by the lectures and textbook up to the specified cutoff. I will not check your answers but rather check to see if you have made a good effort to answer all the questions derived from the material already covered. If you have a reasonable answer for each question checked you will get full points. If you have not made an effort to answer all the questions reviewed you will get a zero.\nUnit 3 exam 25%: Unit 3 comprehension is better checked through an in-class exam. The exam will be open notes but closed book and will take place during the normal lab session that week.\nData Camp labs 5%: A number of labs on the website Data Camp will be assigned to you; these labs are pass/fail and you will receive full credit if you complete each of the labs by the specified due date.\nSyllabus quiz 1%: A short quiz after the first class regarding the course requirements.\nPlacement exams 1%: A short quiz that you will take at both the start and end of class to measure how much pre-knowledge you have and how much knowledge you have gained during the course. Exams are pass/fail depending on if you complete them or not\nAttendance & warmup quizzes: I want you to attend class - there are a lot of studies that show those who attend class and pay attention, learn more and have higher grades. However, I am not your parent. So, it is up to you if you want to be assessed for attendance. Attendance is therefore optional, but, you can sign a pledge at the start of class promising to attend all classes and stay for the entire class (you can miss up to one class without an excuse) and if you complete the pledge, you will receive an extra 1% of extra credit on your final grade. However, if you fail the pledge, you will receive -1% grade deduction. Similarly, the beginning of the class warmup quizzes are partially for me to see how well you have understood the textbook materials, but it is also an incentive for you to read and understand the textbook chapters before class. I will not record your grade on the quizzes. However, you can also sign a pledge to count your warmup quizzes as part of your grade. If you agree to be assessed on the warmup quizzes, you must maintain at least a 75% average on the quizzes (I will drop your lowest quiz score). If you do, you will receive 2% extra credit on your final grade. If you sign the pledge but fail to maintain the average, you will be deducted 2% from your final grade."
  },
  {
    "objectID": "index.html#lateness-policy",
    "href": "index.html#lateness-policy",
    "title": "Course Introduction",
    "section": "Lateness policy",
    "text": "Lateness policy\nSince the course moves very quickly, if you are submitting work late that means you are falling behind on other material and it may be difficult for you to recover. Therefore, I have a fairly strict lateness policy.\n\nAll major assignments are due at 11:59:00 pm. Not 11:59:01 or 11:59:31.\nIf it is later than 11:59:00 pm, then the assignment will be assessed a 5% lateness penalty\nIf it is later than 12:29:00 am, then the assignment will be assessed a 10% lateness penalty\nIf it is later than 11:59:00 pm the next day, the assignment will be assessed a 50% lateness penalty\nIf it is later than 2 days from the due date, I will no longer accept the assignment\n\n\nPlease be sure to check that your homework is complete and make sure to submit it a few minutes early. You can submit multiple times on Teams so make sure you have a nearly complete version uploaded even if you want to keep working on it right up to the deadline. I will not be sympathetic to messages that complain of computer problems when you are trying to submit for the first time at 11:58:51 pm."
  },
  {
    "objectID": "index.html#contact-policy",
    "href": "index.html#contact-policy",
    "title": "Course Introduction",
    "section": "Contact Policy",
    "text": "Contact Policy\n\nI usually try to reply to emails on the same day (though not in the evenings), however responses may be slower on the weekend\nDo not message me 2 hours before a homework is due and expect an immediate response!\nFor general questions, such as how should one interpret a question on a homework or quiz, you should ask the question in the Discussion section of the Teams site. That way others can benefit from the response or someone else may be able to answer more quickly than I can. If you email me a question that really belongs in the help channel I will ask you to repost it.\nMany questions can be answered by carefully checking the class website or reviewing the materials in on Teams. If in doubt, though, feel free to ask."
  },
  {
    "objectID": "index.html#chatgpt-and-similar-policy",
    "href": "index.html#chatgpt-and-similar-policy",
    "title": "Course Introduction",
    "section": "ChatGPT (and similar) Policy",
    "text": "ChatGPT (and similar) Policy\nUnless otherwise specified on an assignment, you may use ChatGPT as much as you wish. However, most of the assignments in this class are not very amenable to ChatGPT help. Your main grade will be understanding, annotating, and interpreting your output. Producing the output is the very simple minimal requirement. Also, ChatGPT often makes many coding mistakes. If you rely too much on ChatGPT to code, you will not understand when it makes mistakes and why it makes mistakes."
  },
  {
    "objectID": "index.html#academic-dishonesty-policy",
    "href": "index.html#academic-dishonesty-policy",
    "title": "Course Introduction",
    "section": "Academic Dishonesty Policy",
    "text": "Academic Dishonesty Policy\nDon’t cheat. Don’t be that person. Yes, you. You know exactly what I’m talking about too.\nMore specifically, you are expected to strictly adhere to the Duke Kunshan University Community Standard in all of your work and participation, and violations will be enforced. More details can be found here.\nAll work must be done exclusively by the individual to whom it has been assigned. Any instance of copying code or analysis from others is considered cheating. You may exchange general ideas about how to complete the assignment but you must write your own code and also write your own analysis.\nIt may sound cliché to say, but if you cheat and borrow others code or answers you are only cheating yourself; you will not learn how to do statistics and doing so will mean you will do worse on the midterm and the final anyway. Cheating is ultimately self-defeating so for both of our benefit, please, don’t do it. If you are having trouble completing the assignment and feel tempted to cheat, please contact me directly instead with the difficulties you are having."
  },
  {
    "objectID": "index.html#disabilities-policy",
    "href": "index.html#disabilities-policy",
    "title": "Course Introduction",
    "section": "Disabilities Policy",
    "text": "Disabilities Policy\nIf you need an accommodation due to a disability, you should not hesitate to request one. The process is that requests should be sent to the Dean of Undergraduate Studies, who will contact me with recommended type of accommodation that is needed. You do not need to disclose your reason for requesting an accommodation with me, and asking through the Dean of Undergraduate Studies helps make things official for both you and me."
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Unit 1 homework (due Sunday, November 2 at 23:59:00)\n\nUnit 1 homework\n[Unit 1 homework sample solutions]\n\n\n\nUnit 2 homework (due Sunday, November 16 at 23:59:00)\n\nUnit 2 homework\n[Unit 2 homework sample solutions]\n\n\n\nUnit 3 exam (Thursday, November 27 from 6:00 pm to 7:15 pm)\n\n\nFinal project (due Wednesday, December 10 at 23:59:00)\n\n[Final project]"
  },
  {
    "objectID": "lectures/lecture.1.2.html#distribution-of-common-quantities",
    "href": "lectures/lecture.1.2.html#distribution-of-common-quantities",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Distribution of common quantities",
    "text": "Distribution of common quantities\nMany phenomena in nature have a relatively easily guessed distribution characteristics\n\nWhat is the distribution of length of rivers in the U.S.?\nWhat is the distribution of width of flower sepals?\nWhat is the distribution of life expectancy across countries in 2007?\n\nFeatures to guess:\n\nShape\nCenter\nSpread"
  },
  {
    "objectID": "lectures/lecture.1.2.html#graphs-of-common-quantities",
    "href": "lectures/lecture.1.2.html#graphs-of-common-quantities",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Graphs of common quantities",
    "text": "Graphs of common quantities\n\nLength of rivers in the U.S.\n\n\n\n\n\n\n\n\n\n\nFlower sepal width\n\n\n\n\n\n\n\n\n\n\nLife expetancy in 2007"
  },
  {
    "objectID": "lectures/lecture.1.2.html#height-activity",
    "href": "lectures/lecture.1.2.html#height-activity",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Height activity",
    "text": "Height activity\nWhat do you expect the shape, center, and spread of class height to be? Why? Write down with your partner your guesses."
  },
  {
    "objectID": "lectures/lecture.1.2.html#height-distribution",
    "href": "lectures/lecture.1.2.html#height-distribution",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Height distribution",
    "text": "Height distribution"
  },
  {
    "objectID": "lectures/lecture.1.2.html#closing-thoughts",
    "href": "lectures/lecture.1.2.html#closing-thoughts",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Closing thoughts",
    "text": "Closing thoughts\n\nMany distributions can be guessed in advanced based on the data generating process\nYou should have at least a guess as to what the distribution is before starting your exploratory data analysis\nThink carefully about what your variable is actually measuring\nCharacteristics of distributions are summaries of the data, almost always obscure features of the data\nDon’t mislead your readers!!"
  },
  {
    "objectID": "lectures/lecture.2.1.html#exercise-1",
    "href": "lectures/lecture.2.1.html#exercise-1",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Exercise 1",
    "text": "Exercise 1\nIn your pairs, try to think of two variables that, in the real world, that might have this correlation for each of the following correlations. Try to think of a few examples for each correlation.\n\n0.95\n0.75\n0.5\n0.25\n0.0\n-0.25\n-0.5\n-0.75\n-0.95\n\nPick a few of these and draw by hand what you expect these graphs to look like."
  },
  {
    "objectID": "lectures/lecture.2.1.html#exercise-2",
    "href": "lectures/lecture.2.1.html#exercise-2",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\nSeattle"
  },
  {
    "objectID": "lectures/lecture.2.1.html#viewing-an-example-relationship",
    "href": "lectures/lecture.2.1.html#viewing-an-example-relationship",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Viewing an example relationship",
    "text": "Viewing an example relationship\n\nFirst, what is our expectation about the relationship between bedrooms and square feet?\nDirection?\nForm?\nStrength?\nOutliers?"
  },
  {
    "objectID": "lectures/lecture.2.1.html#bedrooms-and-square-feet---direction",
    "href": "lectures/lecture.2.1.html#bedrooms-and-square-feet---direction",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - direction",
    "text": "Bedrooms and square feet - direction\nA scatterplot is the easiest way to check for direction. In this case, the direction is obvious"
  },
  {
    "objectID": "lectures/lecture.2.1.html#bedrooms-and-square-feet---form",
    "href": "lectures/lecture.2.1.html#bedrooms-and-square-feet---form",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - form",
    "text": "Bedrooms and square feet - form"
  },
  {
    "objectID": "lectures/lecture.2.1.html#bedrooms-and-square-feet---strength",
    "href": "lectures/lecture.2.1.html#bedrooms-and-square-feet---strength",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - strength",
    "text": "Bedrooms and square feet - strength\n\n\n\n\n\n\n\n\n\nCorrelation as a measure of strength\n\n\n\n\n\n\n\n\nThis correlation is a little weaker than perhaps what we expected\n\nIn general, mechanically generated processes with little noise can have very high correlations\nMost correlations of social or real world processes rarely have above moderate correlation due to noise"
  },
  {
    "objectID": "lectures/lecture.2.1.html#bedrooms-and-square-feet---outlier",
    "href": "lectures/lecture.2.1.html#bedrooms-and-square-feet---outlier",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - outlier",
    "text": "Bedrooms and square feet - outlier\nAgain, we do not have a rule for selecting outliers other than to observe them on the scatterplot. In this case, there is one very obvious value far from other values\n\n\n\n\n\n\n\n\nTo investigate if this outlier matters, we can check some other values of the observation.\n\n\n\n\n\n\n\n\nWhat kind of outlier do you think this is? Why?\n\nOutlier - actual observation\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\n\n\n\nHouse details\n\n\n\n\nData with no outlier\n\n\n\n\n\n\n\n\n\n\nDescribing the association\n\nDirection - positive\nForm - linear\nStrength - moderate/strong\nOutliers - one possible\n\nOutlier:"
  },
  {
    "objectID": "lectures/lecture.2.1.html#does-bed-and-square-feeet-relationship-match-expectations",
    "href": "lectures/lecture.2.1.html#does-bed-and-square-feeet-relationship-match-expectations",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Does bed and square feeet relationship match expectations?",
    "text": "Does bed and square feeet relationship match expectations?\n\nSeems to, more or less, the larger the house, the more bedrooms, so the relationship is positive\nThe relationship is fairly linear, indicating a strong relationship\nRelationship is moderate\nOutliers don’t seem like it is a problem\n\nHowever….\n\nWhat are some possible lurking variables that influence the relationship of bedrooms and bathrooms?"
  },
  {
    "objectID": "lectures/lecture.2.1.html#relationship-reexpressed",
    "href": "lectures/lecture.2.1.html#relationship-reexpressed",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Relationship reexpressed?",
    "text": "Relationship reexpressed?\nOne final issue to consider is if this relationship should be reexpressed - made more linear.\n\n\n\n\n\n\n\n\n\nSeems clearer\n\n\nYour turn\nWith your partner, develop some expectations about some of the variables in the kc.houses dataset might be related.\nVariables:\n\n\n\n\n\n\n\n\nWhat to do with your partner:\n\nWrite down an interesting question we think we might be able to answer by examining a relationship in this dataset\nChoose the variables that you think might be able to answer this question\nWrite down what you expect the relationship to be between these two variables based on any prior knowledge\nDecide which variable is the response variable and which is the predictor variable\nMake a scatterplot using one of the codeblocks in the previous section and identify the features of the association"
  },
  {
    "objectID": "rmanual.html",
    "href": "rmanual.html",
    "title": "R Manual",
    "section": "",
    "text": "credit to Maisie Zhang, the first Stats 101 head tutor, for compiling this guide."
  },
  {
    "objectID": "rmanual.html#install-software",
    "href": "rmanual.html#install-software",
    "title": "R Manual",
    "section": "Install software",
    "text": "Install software\nNote that you must install R before RStudio and both programs are required\nR statistical package\nR Studio user interface"
  },
  {
    "objectID": "rmanual.html#notes-on-use",
    "href": "rmanual.html#notes-on-use",
    "title": "R Manual",
    "section": "Notes on use",
    "text": "Notes on use\n\nCase: does differentiate between uppercase and lowercase\nVariable name: consists of letters, numbers and the dot or underline characters, and starts with a letter or the dot not followed by a number\nPunctuation: only English punctuation is accepted\nPound sign: # leads a comment line\nDollar sign: $ extracts elements by name from a named list and is often used as &lt;data.frame&gt;$&lt;column&gt;\nPercent sign: % is not accepted as percentage and percentage needs to be expressed as fraction\nAssignment operator: &lt;- (left arrow) assigns a value to a name\n\nExample: x &lt;- 2 + 3\n\nTo R, the result is a vector, even though for simple calculations the vector has only one element\nPipe operator: %&gt;% indicates that you are passing the result of one function to the next function. Requires the tidyverse package"
  },
  {
    "objectID": "rmanual.html#install-packages",
    "href": "rmanual.html#install-packages",
    "title": "R Manual",
    "section": "Install packages",
    "text": "Install packages\nR by default comes with many built-in tools however the real power of the program comes from the packages others have written to extend R.\nTo install a new package, there are two options:\n\nUse the command line:\n\ninstall.packages (\"ggplot2\")\n\nUse the user interface:\n\n\nClick Tools - Install Packages…\nType ggplot2 in Packages blank\nClick Install\n\nthen, on the command line, type:\nlibrary(ggplot2)\n\nYou only need to install a package once. However, each time you restart R, you must load the library again using the library() command"
  },
  {
    "objectID": "rmanual.html#keyboard-shortcuts-tips",
    "href": "rmanual.html#keyboard-shortcuts-tips",
    "title": "R Manual",
    "section": "Keyboard shortcuts & tips",
    "text": "Keyboard shortcuts & tips\n\nNavigate command history: Up / Down\nClear console: Ctrl + L\nChange font size: click Tools - Global Options… - Appearance - Editor Font size"
  },
  {
    "objectID": "rmanual.html#basic-functions",
    "href": "rmanual.html#basic-functions",
    "title": "R Manual",
    "section": "Basic functions",
    "text": "Basic functions\nUsually in this class we will be using dpylr package to do most calculations and data manipulation. But in some cases you may find it faster to use the base R functions.\nhelp() : the primary interface to the help systems\n\nexample: help(mean)\n\n\nA more convenient way is to type just ? and the function name. For example, help(mean) and ?mean work the same\n\nc(): combines values into a vector or list\n\nresult &lt;- c(1, 2)\n\nresult\n\n[1] 1 2\n\n\nmean(x, na.rm = FALSE): arithmetic mean\n\nmean(mtcars$mpg)\n\n[1] 20.09062\n\n\nsd(x, na.rm = FALSE): sample standard deviation which uses denominator n-1\n\nsd(mtcars$mpg)\n\n[1] 6.026948\n\n\n\nNote this function does NOT calculate population SD but the sample SD\n\nsummary(): for numeric variables, returns minimum, 1st quartile, median, mean, 3rd quartile, and maximum; for categorical variables, returns counts of all categories\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90"
  },
  {
    "objectID": "rmanual.html#piping",
    "href": "rmanual.html#piping",
    "title": "R Manual",
    "section": "Piping",
    "text": "Piping\nThe usual way to operate on data in this class is with pipes %&gt;%; a pipe will carry over the result of one calculation to the following calculation. You can create arbitrarily long computation chains that can save you a lot of typing over using the built-in R functions.\nTo do some of the calculations in the following sections, you can do instead:\nmean(mtcars$mpg) piped version:\n\nmtcars %&gt;% \n  summarize(mean = mean(mpg))\n\n      mean\n1 20.09062\n\n\nWe can combine the mean() and the sd() function in one command like the following:\n\nmtcars %&gt;% \n  summarize(mean = mean(mpg), sd = sd(mpg))\n\n      mean       sd\n1 20.09062 6.026948"
  },
  {
    "objectID": "rmanual.html#importing-datasets",
    "href": "rmanual.html#importing-datasets",
    "title": "R Manual",
    "section": "Importing datasets",
    "text": "Importing datasets\nAll datasets in this class are in .csv format (comma separated values). To import a .csv dataset, you need to:\n\nClick File - Import Dataset - From Text (base)…\nSelect the file\nMake sure Heading: Yes\nna.strings: if the dataset has any missing values, enter the code for missing values\nStrings as factors: make sure this is ticked\n\n\nIn R, categorical variables are represented as factors. So be careful here as some string values that are simply ID variables should not be classified as factors. There are a number of R commands that can convert between strings and factors later if you need to clean up your dataset after importing it."
  },
  {
    "objectID": "rmanual.html#removing-datasets",
    "href": "rmanual.html#removing-datasets",
    "title": "R Manual",
    "section": "Removing datasets",
    "text": "Removing datasets\nrm(mtcars)\n\nThe datasets we work with in R are not very large so I don’t recommend removing any datasets from memory. Once you delete it, there is no way to recover the dataset."
  },
  {
    "objectID": "rmanual.html#rename-a-dataset",
    "href": "rmanual.html#rename-a-dataset",
    "title": "R Manual",
    "section": "Rename a dataset",
    "text": "Rename a dataset\n\nmpgcars &lt;- mtcars"
  },
  {
    "objectID": "rmanual.html#dealing-with-factors",
    "href": "rmanual.html#dealing-with-factors",
    "title": "R Manual",
    "section": "Dealing with factors",
    "text": "Dealing with factors\nR stores categorical variables as a type of variable called a factor. This is different than variables stored as simply text - R will not perform any operations in variables stored simply as text.\nTo convert between text and factors, you can use the following commands:\n\nfruits &lt;- c(\"apple\", \"pear\", \"banana\")\n\n# Will not work properly because the variable is a text variable\nsummary(fruits)\n\n   Length     Class      Mode \n        3 character character \n\n# The factor command will convert it to a factor so R can interpret the contents\nsummary(factor(fruits))\n\n apple banana   pear \n     1      1      1 \n\n\nIt is not a commonly used function, but if for some reason you need to convert back to text, you can use as.character() or as.numeric()\n\nfruits &lt;- factor(fruits)\n\n# Converts a factor back to text. as.numeric() does the same if the factor names are numbers\nas.character(fruits)\n\n[1] \"apple\"  \"pear\"   \"banana\""
  },
  {
    "objectID": "rmanual.html#select-and-subset",
    "href": "rmanual.html#select-and-subset",
    "title": "R Manual",
    "section": "Select and subset",
    "text": "Select and subset\n\nSelecting columns\n\nmtcars %&gt;% \n  select(mpg)\n\n                     mpg\nMazda RX4           21.0\nMazda RX4 Wag       21.0\nDatsun 710          22.8\nHornet 4 Drive      21.4\nHornet Sportabout   18.7\nValiant             18.1\nDuster 360          14.3\nMerc 240D           24.4\nMerc 230            22.8\nMerc 280            19.2\nMerc 280C           17.8\nMerc 450SE          16.4\nMerc 450SL          17.3\nMerc 450SLC         15.2\nCadillac Fleetwood  10.4\nLincoln Continental 10.4\nChrysler Imperial   14.7\nFiat 128            32.4\nHonda Civic         30.4\nToyota Corolla      33.9\nToyota Corona       21.5\nDodge Challenger    15.5\nAMC Javelin         15.2\nCamaro Z28          13.3\nPontiac Firebird    19.2\nFiat X1-9           27.3\nPorsche 914-2       26.0\nLotus Europa        30.4\nFord Pantera L      15.8\nFerrari Dino        19.7\nMaserati Bora       15.0\nVolvo 142E          21.4\n\n\nTo select multiple columns you can do:\n\nmtcars %&gt;% \n  select(c(mpg, cyl))\n\n                     mpg cyl\nMazda RX4           21.0   6\nMazda RX4 Wag       21.0   6\nDatsun 710          22.8   4\nHornet 4 Drive      21.4   6\nHornet Sportabout   18.7   8\nValiant             18.1   6\nDuster 360          14.3   8\nMerc 240D           24.4   4\nMerc 230            22.8   4\nMerc 280            19.2   6\nMerc 280C           17.8   6\nMerc 450SE          16.4   8\nMerc 450SL          17.3   8\nMerc 450SLC         15.2   8\nCadillac Fleetwood  10.4   8\nLincoln Continental 10.4   8\nChrysler Imperial   14.7   8\nFiat 128            32.4   4\nHonda Civic         30.4   4\nToyota Corolla      33.9   4\nToyota Corona       21.5   4\nDodge Challenger    15.5   8\nAMC Javelin         15.2   8\nCamaro Z28          13.3   8\nPontiac Firebird    19.2   8\nFiat X1-9           27.3   4\nPorsche 914-2       26.0   4\nLotus Europa        30.4   4\nFord Pantera L      15.8   8\nFerrari Dino        19.7   6\nMaserati Bora       15.0   8\nVolvo 142E          21.4   4\n\n\nMore details on advanced select commands can be found here:\nSelecting columns documentation\n\n\nSelecting rows\n\nmtcars %&gt;% \n  filter(cyl==6)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n\nAs with the column example, multiple criteria can be used:\n\nmtcars %&gt;% \n  filter(cyl==6 & hp==110)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n\nMore details on advanced filter commands can be found here:\nFiltering rows documentation"
  },
  {
    "objectID": "rmanual.html#recoding-variables",
    "href": "rmanual.html#recoding-variables",
    "title": "R Manual",
    "section": "Recoding variables",
    "text": "Recoding variables\nThe main dplyr verb for recoding a variable is mutate. With mutate, you can either simply rescale a variable or do more complex transformations, such as the following:\n\n# Weight is defined as 1000s of pounds; a ton is 2000 pounds\nmtcars &lt;- mtcars %&gt;% \n  mutate(tons = (wt / 2000) * 1000)\n\nA more complex transformation using the helper function case_when:\n\n# Note we need to convert the strings to a factor after the case_when call\nmtcars &lt;- mtcars %&gt;% \n  mutate(cartype = case_when(\n    tons &lt; 1 ~ \"light\",\n    tons &gt;= 1 & tons &lt; 2 ~ \"medium\",\n    tons &gt; 2 ~ \"heavy\"\n  )) %&gt;% \n  mutate(cartype = factor(cartype))\n\nMore details on advanced mutate commands can be found here:\nMutate documentation\nMore details on advanced case_when commands can be found here:\nCase when documentation"
  },
  {
    "objectID": "rmanual.html#missing-values",
    "href": "rmanual.html#missing-values",
    "title": "R Manual",
    "section": "Missing values",
    "text": "Missing values\nMany datasets have missing values (for a variety of reasons). It is always important to check if your dataset has any missing values (in R, these are stored by default as NA).\nTo check the total number of missing values, we can use the following:\n\nmtcars.missing &lt;- mtcars\n\n# Make the first observation missing\nmtcars.missing[1,1] &lt;- NA\n\ncolSums(is.na (mtcars.missing))\n\n    mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear \n      1       0       0       0       0       0       0       0       0       0 \n   carb    tons cartype \n      0       0       0 \n\n\nMost functions in R have a flag to remove missing values. If there is a missing value, the function will return an error if the flag is not set.\n\nmean(mtcars.missing$mpg)\n\n[1] NA\n\nmean(mtcars.missing$mpg, na.rm=TRUE)\n\n[1] 20.06129\n\n\n\nSometimes cases with missing values suggest that certain types of observations are not stored properly. It is always important to check if there are any patterns in the observations with missing values before removal."
  },
  {
    "objectID": "rmanual.html#helpful-summary-functions",
    "href": "rmanual.html#helpful-summary-functions",
    "title": "R Manual",
    "section": "Helpful summary functions",
    "text": "Helpful summary functions\n\nSummarizing data\nThe simpliest way to summarize data is the summary() command.\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nTo develop more complex summary statistics, you can use longer piped structures from dplyr as follows:\n\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(mean = mean(wt), n = n())\n\n# A tibble: 3 × 3\n    cyl  mean     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     4  2.29    11\n2     6  3.12     7\n3     8  4.00    14\n\n\nTo view more samples of the summarize() function, see here:\nSummarize documetation\n\n\nFinding normal distribution & quantile information\nIf you want to find out details of the normal distribution, there are a suite of function in R that can report the quantile of a specific number of standard deviations from the center of a normal distribution or the reverse.\n\n# What is the amount of area under the normal curve cumulative up to +2 sd from the mean \npnorm(2, mean=0, sd=1)\n\n[1] 0.9772499\n\n# At how many standard deviations away from the mean is 97.5% of the area under the normal curve\nqnorm(0.975, mean=0, sd=1)\n\n[1] 1.959964\n\n\nIf you have an arbitrary distribution (that may not necessarily be normally distributed), you can find a quantile for a given percentage of area under the curve with the quantile() function.\n\n# generate a uniform distribution\ndist&lt;-runif(1000, min=0, max=1)\n\n# theoretically, the quantile should match the probability for a uniform distribution\nquantile(dist, probs=0.5)\n\n      50% \n0.4951526"
  },
  {
    "objectID": "rmanual.html#frequency-table-and-contingency-table",
    "href": "rmanual.html#frequency-table-and-contingency-table",
    "title": "R Manual",
    "section": "Frequency table and contingency table",
    "text": "Frequency table and contingency table\nCreating a frequency table is quite easy in R.\n\ntable(mtcars$cyl, mtcars$gear)\n\n   \n     3  4  5\n  4  1  8  2\n  6  2  4  1\n  8 12  0  2\n\naddmargins(table(mtcars$cyl, mtcars$gear))\n\n     \n       3  4  5 Sum\n  4    1  8  2  11\n  6    2  4  1   7\n  8   12  0  2  14\n  Sum 15 12  5  32\n\n\nSo is making a contingency table.\n\nprop.table(table(mtcars$cyl, mtcars$gear))\n\n   \n          3       4       5\n  4 0.03125 0.25000 0.06250\n  6 0.06250 0.12500 0.03125\n  8 0.37500 0.00000 0.06250\n\naddmargins(prop.table(table(mtcars$cyl, mtcars$gear)))\n\n     \n            3       4       5     Sum\n  4   0.03125 0.25000 0.06250 0.34375\n  6   0.06250 0.12500 0.03125 0.21875\n  8   0.37500 0.00000 0.06250 0.43750\n  Sum 0.46875 0.37500 0.15625 1.00000\n\n\nChanging the method of calculating the margins can be done with the margin option.\n\nprop.table(table(mtcars$cyl, mtcars$gear), margin=1)\n\n   \n             3          4          5\n  4 0.09090909 0.72727273 0.18181818\n  6 0.28571429 0.57142857 0.14285714\n  8 0.85714286 0.00000000 0.14285714\n\nprop.table(table(mtcars$cyl, mtcars$gear), margin=2)\n\n   \n             3          4          5\n  4 0.06666667 0.66666667 0.40000000\n  6 0.13333333 0.33333333 0.20000000\n  8 0.80000000 0.00000000 0.40000000"
  },
  {
    "objectID": "rmanual.html#histogram",
    "href": "rmanual.html#histogram",
    "title": "R Manual",
    "section": "Histogram",
    "text": "Histogram\nHistograms are quite easy using ggplot.\n\nggplot(mtcars, aes(x=wt)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nDetails on how to modify the graphical parameters of geom_histogram() can be found here:\nHistogram documetation\n\nRemember, histograms should only be use for quantitative data. Even if a categorical variable is numeric (cylinders, for example), you should represent that variable with a bar chart."
  },
  {
    "objectID": "rmanual.html#boxplot",
    "href": "rmanual.html#boxplot",
    "title": "R Manual",
    "section": "Boxplot",
    "text": "Boxplot\nBoxplots are useful if you want to compare the distribution of a variable across several different categories. For example, boxplots are a good way to compare the distribution of car weight by number of cylinders in a car.\n\nggplot(mtcars, aes(x=factor(cyl), y=wt)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nThe grouping variable x must be categorical in the box plot call\n\nDetails on how to modify the graphical parameters of geom_boxplot() can be found here:\nBoxplot documetation"
  },
  {
    "objectID": "rmanual.html#bar-chart",
    "href": "rmanual.html#bar-chart",
    "title": "R Manual",
    "section": "Bar chart",
    "text": "Bar chart\nBar charts are the best way to display the distribution of categorical variables. Since we generally don’t assign importance to the order or distance between categories, we can only simply show the count of each category independently.\n\nggplot(mtcars, aes(x=factor(cyl))) +\n  geom_bar()\n\n\n\n\n\n\n\n\nDetails on how to modify the graphical parameters of geom_bar() can be found here:\nBar chart documetation"
  },
  {
    "objectID": "rmanual.html#qq-plot-in-ggplot",
    "href": "rmanual.html#qq-plot-in-ggplot",
    "title": "R Manual",
    "section": "QQ Plot in ggplot",
    "text": "QQ Plot in ggplot\nQQ plots are useful to determine if a distribution is normal shaped. While we won’t discuss it in this class, it may be helpful for certain projects you work on. Similar to other singe variable ggplots, you can create one as follows:\n\nggplot(mtcars, aes(sample=wt)) +\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\nInformation on how to interpret these plots can be found in Chapter 5 of the textbok.\nDetails on how to modify the graphical parameters of geom_qq() can be found here:\nQQ plot documetation"
  },
  {
    "objectID": "rmanual.html#scatterplot-in-ggplot",
    "href": "rmanual.html#scatterplot-in-ggplot",
    "title": "R Manual",
    "section": "Scatterplot in ggplot",
    "text": "Scatterplot in ggplot\nScatterplots are the most common way to display a two-variable relationship and are one of the most common graphical displays.\n\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nRemember that your response variable should always be on the y axis on a scatterplot\n\nIt is relatively common to add a line of best fit into a scatterplot to summarize the relationship between the two variables. You can do so by using the geom_smooth() function.\n\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE)\n\n\n\n\n\n\n\n\nThere are a number of different options for how to choose a best fit line, remember to select the one that you think best represents the relationship between the two variables.\nDetails on how to modify the graphical parameters of geom_point() and geom_smooth() can be found here:\nScatterplot documetation\nSmoother documetation"
  },
  {
    "objectID": "rmanual.html#correlation-in-ggcorrplot",
    "href": "rmanual.html#correlation-in-ggcorrplot",
    "title": "R Manual",
    "section": "Correlation in ggcorrplot",
    "text": "Correlation in ggcorrplot\nOne helpful graphical display to quickly summarize the relationship between many variables is a correlation plot. It displays the correlation between the variables you specify in your dataset. First, you need to work out which variables you want to include in the correlation matrix.\n\nmtcars.subset &lt;- mtcars %&gt;% \n  select(c(mpg, cyl, wt, disp))\n\n# use = complete.obs here in case of missing values\nmtcars.cor &lt;- cor(mtcars.subset, use=\"complete.obs\")\n\nmtcars.cor\n\n            mpg        cyl         wt       disp\nmpg   1.0000000 -0.8521620 -0.8676594 -0.8475514\ncyl  -0.8521620  1.0000000  0.7824958  0.9020329\nwt   -0.8676594  0.7824958  1.0000000  0.8879799\ndisp -0.8475514  0.9020329  0.8879799  1.0000000\n\n\nThen simply use the ggcorrplot() function.\n\nggcorrplot(mtcars.cor)\n\n\n\n\n\n\n\n\nDetails on how to modify the graphical parameters of ggcorrplot() can be found here:\nggcorrplot documetation"
  },
  {
    "objectID": "rmanual.html#adding-features-to-ggplots",
    "href": "rmanual.html#adding-features-to-ggplots",
    "title": "R Manual",
    "section": "Adding features to ggplots",
    "text": "Adding features to ggplots\nYou can modify ggplots to add many different features. Below is a short list of basic things you can add but this is not by far a complete list.\n\nChange the colors and lines\n\nggplot (mtcars, aes(x = wt)) + \n  geom_histogram(fill = \"#FF6666\", alpha = 0.8, color = \"black\", linetype = \"dashed\", size = 1)\n\n\n\n\n\n\n\n\n\n\nAdd labels and title\n\nggplot(mtcars, aes (x = wt)) + \n  geom_histogram() +\n  xlab(\"Weight\") +\n  ylab (\"Count\") + \n  ggtitle(\"Weight of Cars in the MPG Dataset\")\n\n\n\n\n\n\n\n\n\n\nAdd lines to the graph\n\nggplot(mtcars, aes(x = wt)) + \n  geom_histogram () + \n  geom_vline(aes(xintercept = mean(wt))) + \n  geom_hline(aes(yintercept = 3))"
  },
  {
    "objectID": "rmanual.html#arrange-multiple-ggplot-plots",
    "href": "rmanual.html#arrange-multiple-ggplot-plots",
    "title": "R Manual",
    "section": "Arrange multiple ggplot plots",
    "text": "Arrange multiple ggplot plots\nOften it can be helpful to combine many plots of the same time (histograms, for example) into one larger plot. To do this, you will need to install the gridExtra package.\n\nlibrary(gridExtra)\n\np1 &lt;- ggplot(mtcars, aes(x = wt)) + \n  geom_histogram ()\n\np2 &lt;- ggplot(mtcars, aes(x = hp)) + \n  geom_histogram ()\n\ngrid.arrange(p1, p2)\n\n\n\n\n\n\n\n\nDetails on how to modify the graphical parameters of grid.arrange() can be found here:\ngridExtra documentation"
  },
  {
    "objectID": "rmanual.html#correlation",
    "href": "rmanual.html#correlation",
    "title": "R Manual",
    "section": "Correlation",
    "text": "Correlation\nThe default dplyr method of calculating a correlation can only do a two variable comparison.\n\nmtcars %&gt;% \n  summarize(cor(wt, mpg))\n\n  cor(wt, mpg)\n1   -0.8676594\n\n\nTo calculate the correlation between many variables, we can use the advanced package corrr as follows:\n\nlibrary(corrr)\n\nmtcars %&gt;% \n  select(c(mpg, disp, wt)) %&gt;% \n  correlate()\n\n# A tibble: 3 × 4\n  term     mpg   disp     wt\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 mpg   NA     -0.848 -0.868\n2 disp  -0.848 NA      0.888\n3 wt    -0.868  0.888 NA    \n\n\nThe corrr package has many advanced features and graphical capabilities if you need to calculate something more specific than the correlation matrix of the dataset.\nDetails on corr() can be found here:\ncorrr documentation"
  },
  {
    "objectID": "rmanual.html#simple-regression",
    "href": "rmanual.html#simple-regression",
    "title": "R Manual",
    "section": "Simple regression",
    "text": "Simple regression\n\nCalculating the regression coefficients\nThe standard way to calculate a regression is as follows, with the response variable on the left and the predictor variable on the right:\n\nfit &lt;- lm(data=mtcars, mpg ~ wt)\n\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThere is not a particularly elegant means of calculating regressions using the tidy package, however, it is often useful, particularly when conducting many regressions, to use the broom package to return the regression results in a data frame.\n\nlibrary(broom)\n\ntidy(fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    37.3      1.88      19.9  8.24e-19\n2 wt             -5.34     0.559     -9.56 1.29e-10\n\n\nFor more details on tidy(), you can go here:\ntidy documentation\n\n\nGraphing the residuals\nTo graph the residuals, we can use the augment() function from the broom package to add in the residuals into the results.\n\naug.fit &lt;- augment(fit)\n\naug.fit\n\n# A tibble: 32 × 9\n   .rownames           mpg    wt .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 Mazda RX4          21    2.62    23.3 -2.28  0.0433   3.07 1.33e-2    -0.766 \n 2 Mazda RX4 Wag      21    2.88    21.9 -0.920 0.0352   3.09 1.72e-3    -0.307 \n 3 Datsun 710         22.8  2.32    24.9 -2.09  0.0584   3.07 1.54e-2    -0.706 \n 4 Hornet 4 Drive     21.4  3.22    20.1  1.30  0.0313   3.09 3.02e-3     0.433 \n 5 Hornet Sportabout  18.7  3.44    18.9 -0.200 0.0329   3.10 7.60e-5    -0.0668\n 6 Valiant            18.1  3.46    18.8 -0.693 0.0332   3.10 9.21e-4    -0.231 \n 7 Duster 360         14.3  3.57    18.2 -3.91  0.0354   3.01 3.13e-2    -1.31  \n 8 Merc 240D          24.4  3.19    20.2  4.16  0.0313   3.00 3.11e-2     1.39  \n 9 Merc 230           22.8  3.15    20.5  2.35  0.0314   3.07 9.96e-3     0.784 \n10 Merc 280           19.2  3.44    18.9  0.300 0.0329   3.10 1.71e-4     0.100 \n# ℹ 22 more rows\n\n\nAnd then we can graph the residuals as follows:\n\nggplot(aug.fit, aes(x=.fitted, y=.resid)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(aug.fit, aes(x=.resid)) +\n  geom_histogram()"
  },
  {
    "objectID": "rmanual.html#multiple-regression",
    "href": "rmanual.html#multiple-regression",
    "title": "R Manual",
    "section": "Multiple regression",
    "text": "Multiple regression\nMultiple regression works similarly to two variable regressions. You can easily construct advanced models (including categorical predictors and interaction terms) using the lm() command:\n\n# Three variable regression\nmod1 &lt;- lm(data=mtcars, mpg~disp+wt)\ntidy(mod1)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  35.0      2.16        16.2  4.91e-16\n2 disp         -0.0177   0.00919     -1.93 6.36e- 2\n3 wt           -3.35     1.16        -2.88 7.43e- 3\n\n# Model with a categorical predictor\nmod2 &lt;- lm(data=mtcars, mpg~wt+factor(cyl))\ntidy(mod2)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     34.0      1.89      18.0  6.26e-17\n2 wt              -3.21     0.754     -4.25 2.13e- 4\n3 factor(cyl)6    -4.26     1.39      -3.07 4.72e- 3\n4 factor(cyl)8    -6.07     1.65      -3.67 9.99e- 4\n\n# Model with an interaction term\nmod3 &lt;- lm(data=mtcars, mpg~wt+factor(cyl)+factor(cyl)*wt)\ntidy(mod3)\n\n# A tibble: 6 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        39.6       3.19    12.4   2.06e-12\n2 wt                 -5.65      1.36    -4.15  3.13e- 4\n3 factor(cyl)6      -11.2       9.36    -1.19  2.44e- 1\n4 factor(cyl)8      -15.7       4.84    -3.24  3.22e- 3\n5 wt:factor(cyl)6     2.87      3.12     0.920 3.66e- 1\n6 wt:factor(cyl)8     3.45      1.63     2.12  4.34e- 2\n\n\n\nGraphing partial residual plots\nPartial residual plots help graphically display the relationship between a single predictor variable and the response variable after controlling for, or partialling out, the effect of all other predictor variables. If your regression specification meets all of the conditions for linear regression, this plot will illustrate the independent impact of the predictor variable on the response variable.\nThere are a few packages that can generate these plots, car being perhaps the best supported.\n\nlibrary(car)\n\ncrPlots(mod1, terms = ~ ., layout = NULL)\n\n\n\n\n\n\n\n\nYou can interpret these graphs as displaying the change in MPG from the mean (listed on the y axis) as displ or wt changes over its range. These are a type of marginal plots - it show the marginal impact of the predictor variable on the response variable after controlling for the impact of all other variables.\n\nNote that in the crPlot() call, the terms function is a one-sided formula that specifies a subset of the predictor variables for which you would like to generate plots. One component-plus-residual plot is drawn for each regressor. The default ~ . is to plot all numeric regressors. You can modify this term to subtract terms with the specification terms = ~ . - X3, which would plot against all regressors except for X3, while terms = ~ log(X4) would give the plot for the predictor X4 that is represented in the model by log(X4). If this argument is a quoted name of one of the predictors, the component-plus-residual plot is drawn for that predictor only.\n\n\nFor the layout option, if set to a value like c(1, 1) or c(4, 3), the layout of the graph will have this many rows and columns. If not set, the program will select an appropriate layout. If the number of graphs exceed nine, you must select the layout yourself, or you will get a maximum of nine per page.\n\nFor more details on crPlots(), you can go here:\ncrPlots documentation"
  },
  {
    "objectID": "rmanual.html#confidence-interval-for-proportion",
    "href": "rmanual.html#confidence-interval-for-proportion",
    "title": "R Manual",
    "section": "Confidence interval for proportion",
    "text": "Confidence interval for proportion\nTo calculate the confidence interval of a proportion, there are often several steps involved. First you will need to possibly recode the variable and select the cases of interest, then obtain two properties of the sample (sample proportion, sample standard deviation) and find the appropriate \\(z\\) score (\\(z^*\\)) needed for your confidence interval.\n\nRemember to check the necessary conditions first!\n\n\nRecode variable\nIt is easiet to work with the variable if successes are marked as 1 and failures maked as 0 in the dataset. You may want to recode your variable to make your calculations easier (see Recoding Variables)\n\nmtcars &lt;- mtcars %&gt;% \n  mutate(heavy = case_when(\n    tons &lt; 1.5 ~ 0,\n    tons &gt;= 1.5 ~ 1 \n  ))\n\n\n\nSample proportion and standard deviation\nFirst we need to find the key features of our sample.\n\nheavy.sum.data &lt;- mtcars %&gt;% \n  summarize(prop = mean(heavy),\n            len = length(heavy)) %&gt;% \n  mutate(sd = sqrt(prop*(1-prop)/len))\n\nheavy.sum.data\n\n   prop len         sd\n1 0.625  32 0.08558165\n\n\n\nRemember, the formula for sample sd is \\(\\sigma(\\hat{p})=\\sqrt{\\frac{pq}{n}}\\)\n\n\n\nConfidence interval\nFirst we need to calculate the margin of error (MOE). In the below example we are interested in a 95% confidence interval. qnorm(0.975) finds the appropriate value of \\(z^*\\) .\n\nheavy.sum.data &lt;- heavy.sum.data %&gt;% \n  mutate(moe = sd * qnorm(0.975))\n\nFinally, we both subtract and add the MOE to the sample proportion to find the confidence interval.\n\nheavy.sum.data$prop + c(-heavy.sum.data$moe, heavy.sum.data$moe)\n\n[1] 0.457263 0.792737"
  },
  {
    "objectID": "rmanual.html#confidence-interval-for-mean",
    "href": "rmanual.html#confidence-interval-for-mean",
    "title": "R Manual",
    "section": "Confidence interval for mean",
    "text": "Confidence interval for mean\nThe R procedure for mean confidence intervals is very similar to proportions, differing only in the method of calculating the standard deviation, which must be done using the sd() function.\n\nRemember to check the necessary conditions first!\n\n\nmpg.sum.data &lt;- mtcars %&gt;% \n  summarize(mean = mean(mpg),\n            len = length(mpg),\n            sd = sd(mpg)) %&gt;% \n  mutate(moe = sd * qnorm(0.975),\n         lower.bound = mean - moe,\n         upper.bound = mean + moe)\n\nc(mpg.sum.data$lower.bound, mpg.sum.data$upper.bound)\n\n[1]  8.278024 31.903226"
  },
  {
    "objectID": "rmanual.html#hypothesis-test-for-proportion",
    "href": "rmanual.html#hypothesis-test-for-proportion",
    "title": "R Manual",
    "section": "Hypothesis test for proportion",
    "text": "Hypothesis test for proportion\nIf we are interested in testing whether there are more heavy cars with 6 cylinders compared to the overall proportion of heavy cars, the R code for such a test is relatively straightforward.\n\nRemember to check the necessary conditions first!\n\nFirst, identify the hypotheses and calculate the sample information.\n\\(H_0: 6cyl.heavy.prop = 0.625\\)\n\\(H_a: 6cyl.heavy.prop \\neq 0.625\\)\n\nmean.heavy &lt;- mean(mtcars$heavy)\n\nhyp.test.prop &lt;- mtcars %&gt;% \n  filter(cyl==6) %&gt;% \n  summarize(prop = mean(heavy),\n            len = length(heavy),\n            sd = sqrt(mean.heavy*(1-mean.heavy)/len))\n\n\nNote that the sd is calculated as if the null hypothesis were true; we use the population mean to calculate here.\n\nNext, calculate the \\(z\\) distance between the sample and the population mean.\n\nz.score &lt;- (hyp.test.prop$prop - mean.heavy)/hyp.test.prop$sd\n\nz.score\n\n[1] -0.29277\n\n\nFinally, we find the \\(p\\) value for that difference and compare it to our \\(\\alpha\\) value.\n\n# Note that our test was specified as two sided, therefore we need to multiply by 2\np.value &lt;- (pnorm(-abs(z.score)))*2\n\np.value\n\n[1] 0.7696979\n\n\nThere is a 77% chance that one would observe a difference that large or larger from the sample mean by simple sample variation. In other words, we fail to reject the null hypothesis if our \\(\\alpha\\) is 5%."
  },
  {
    "objectID": "rmanual.html#hypothesis-test-for-mean",
    "href": "rmanual.html#hypothesis-test-for-mean",
    "title": "R Manual",
    "section": "Hypothesis test for mean",
    "text": "Hypothesis test for mean\nHypothesis test for the means are very similar, though in this case we use the sample sd to estimate the population sd.\n\nRemember to check the necessary conditions first!\n\n\nmean.mpg &lt;- mean(mtcars$mpg)\n\nhyp.test.mean &lt;- mtcars %&gt;% \n  filter(cyl==6) %&gt;% \n  summarize(mean = mean(mpg),\n            len = length(mpg),\n            sd = sd(mpg)/sqrt(len))\n\nInstead of calculating a \\(z\\) score, because of the need to correct for small sample bias, we must use the \\(t\\) distribution.\n\nt.score &lt;- (hyp.test.mean$mean - mean.mpg) / hyp.test.mean$sd\ndf.test &lt;- hyp.test.mean$len - 1\n\np.value &lt;- pt(-abs(t.score), df=df.test)*2\n\np.value\n\n[1] 0.5500829\n\n\nThere is a 55% chance that one would observe a difference that large or larger from the sample mean by simple sample variation. In other words, we fail to reject the null hypothesis if our \\(\\alpha\\) is 5%."
  },
  {
    "objectID": "rmanual.html#inference-for-difference-between-two-samples",
    "href": "rmanual.html#inference-for-difference-between-two-samples",
    "title": "R Manual",
    "section": "Inference for Difference between Two Samples",
    "text": "Inference for Difference between Two Samples\nThe R code for \\(t\\) tests is very similar to the code for hypothesis testing.\n\nfourcyl &lt;- mtcars %&gt;% \n  filter(cyl==4) %&gt;% \n  summarize(mean=mean(mpg),\n            len=length(mpg),\n            sd=sd(mpg))\n\neightcyl &lt;- mtcars %&gt;% \n  filter(cyl==8) %&gt;% \n  summarize(mean=mean(mpg),\n            len=length(mpg),\n            sd=sd(mpg))\n\ndiff.sd &lt;- sqrt(fourcyl$sd ^ 2 / fourcyl$len + eightcyl$sd ^ 2 / eightcyl$len)\ndiff.mpg.t  &lt;- (fourcyl$mean - eightcyl$mean) / diff.sd\n\n# Note that the df here is just an approximation; the textbook provides the correct formula.\n# The rule of thumb method from the the textbook recommends using the lowest df of the two samples\n# minus one.\ndiff.mpg.p  &lt;- 2 * pt(-abs(diff.mpg.t), df = min(c(fourcyl$len, eightcyl$len))-1)\n\ndiff.mpg.p\n\n[1] 1.847002e-05\n\n\nIn other words, the odds that, assuming the two samples have the same sample mean, the odds that we would see a difference between the two sample means this large or larger is effectively zero, therefore we can reject the null hypothesis.\nR does have a built-in method to do this calculation but you should understand how to achieve the same result by direct calcuation.\n\nfourcyl.data &lt;- mtcars %&gt;% \n  filter(cyl==4) %&gt;% \n  select(mpg)\n\neightcyl.data &lt;- mtcars %&gt;% \n  filter(cyl==8) %&gt;% \n  select(mpg)\n\nt.test(fourcyl.data$mpg, eightcyl.data, alternative = \"two.sided\" , mu = 0, var.equal = FALSE, conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  fourcyl.data$mpg and eightcyl.data\nt = 7.5967, df = 14.967, p-value = 1.641e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  8.318518 14.808755\nsample estimates:\nmean of x mean of y \n 26.66364  15.10000 \n\n\nDue to the first method using a rule of thumb method for calculating the degrees of freedom, the result is slightly different but very close to the R function result."
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "#| label: setup\n#| include: false\n#| autorun: true\n\ntheme_set(theme_classic())\n\nglobal.data &lt;- read.csv(\"Country_data.csv\")\nclassroster &lt;- read.csv(\"classroster.csv\", fileEncoding=\"UTF-8-BOM\")\n\nglobal.data &lt;- global.data %&gt;% \n  mutate(gdp.capita = World.Bank.Forecast.GDP.PPP. / Population.2023 * 10,\n         hdi.score = HDI) %&gt;% \n  filter(if_all(c(gdp.capita, hdi.score), ~ !is.na(.)))\n\nglobal.data.nooutlier &lt;- global.data %&gt;%\n  filter(gdp.capita &gt; 1)\n\nglobal.data.log &lt;- global.data %&gt;%\n  mutate(log.gdp.capita = log(gdp.capita))\n\nglobal.data.log.nooutlier &lt;- global.data.log %&gt;% \n  filter(log.gdp.capita &gt; -6)\n\nhdi.model &lt;- lm(hdi.score ~ gdp.capita, data=global.data)\nglobal.data.augmented &lt;- augment(hdi.model, global.data)\nhdi.model.tidy &lt;- tidy(hdi.model)\n\nhdi.model.nooutlier &lt;- lm(hdi.score ~ gdp.capita, data=global.data.nooutlier)  \nglobal.data.augmented.nooutlier &lt;- augment(hdi.model.nooutlier, global.data.nooutlier)\nhdi.model.tidy.nooutlier &lt;- tidy(hdi.model.nooutlier)\n\nhdi.model.log &lt;- lm(hdi.score ~ log.gdp.capita, data=global.data.log)\nglobal.data.augmented.log &lt;- augment(hdi.model.log, global.data.log)\nhdi.model.tidy.log &lt;- tidy(hdi.model.log)\n\nhdi.model.log.nooutlier &lt;- lm(hdi.score ~ log.gdp.capita, data=global.data.log.nooutlier)\nglobal.data.augmented.log.nooutlier &lt;- augment(hdi.model.log.nooutlier, global.data.log.nooutlier)\nhdi.model.tidy.log.nooutlier &lt;- tidy(hdi.model.log.nooutlier)"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#line-of-best-fit",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#line-of-best-fit",
    "title": "Simple Linear Regression",
    "section": "Line of best fit",
    "text": "Line of best fit\n\n\n\nHDI definition\n\n\nHow to describe the relationship between GDP per capita and HDI score?\nAs we learned:\n\nDirection\nForm\nStrength\nOutlier\n\nWhat do you expect the relationship between GDP per capita and HDI will be?\n\n#| label: picker0\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#scatterplot-of-gdp-per-capita-and-hdi",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#scatterplot-of-gdp-per-capita-and-hdi",
    "title": "Simple Linear Regression",
    "section": "Scatterplot of GDP per capita and HDI",
    "text": "Scatterplot of GDP per capita and HDI\n\n#| label: fig-hdiscatterplot\n#| caption: \"HDI as a function of GDP/cap\"\nggplot(global.data, aes(x=gdp.capita, y=hdi.score)) +\n  geom_point() +\n  labs(x=\"GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\")\n\n\nSmoother\n\n#| label: fig-hdismoother\n#| caption: \"HDI as a function of GDP/cap with smoother\"\nggplot(global.data, aes(x=gdp.capita, y=hdi.score)) +\n  geom_point() +\n  labs(x=\"GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\") +\n  geom_smooth(se=FALSE)\n\n\n\nTaking a guess\nWhat do you think the intercept and the slope should be for a line of ‘good’ fit?\n\n#| label: picker1\nsample(classroster$name, 1)\n\n\n#| label: fig-guessslope\n#| caption: \"HDI as a function of GDP/cap\"\nggplot(global.data, aes(x=gdp.capita, y=hdi.score)) +\n  geom_point() +\n  labs(x=\"GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\") +\n  geom_abline(intercept = 0, slope=0, color=\"blue\")\n\n\n\nLeast squares line\n\nSlope: 0.4498406, intercept: 0.6025812\n\n\n#| label: fig-leastsquares\n#| caption: \"HDI as a function of GDP/cap with least squares line\"\nggplot(global.data, aes(x=gdp.capita, y=hdi.score)) +\n  geom_point() +\n  labs(x=\"GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\") +\n  geom_smooth(method=\"lm\", se=FALSE)"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#linear-model-1",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#linear-model-1",
    "title": "Simple Linear Regression",
    "section": "Linear model",
    "text": "Linear model\nIt’s better if we come up with a more formal model: \\(\\hat{y}= b_0 + b_1x\\)\n\\(\\hat{y}\\) is our predicted value \\(b_0\\) is the \\(y\\) intercept - the value when \\(x\\) is 0 \\(b_1\\) is the slope\n\nHelps with predictions\n\nFor values not in the sample, we can estimate their HDI score\n\nHelps assess model fit - we can compare different lines more easily\n\nMore specifically we can calculate the residuals\nResiduals are difference between our line and the actually observed value - how much our line ‘missed’ by\n\n\n\nLinear model for our data\n\\(\\hat{y}= 0.6025812 + 0.4498406x\\)\n\n#| label: fig-leastsquares2\n#| caption: \"HDI as a function of GDP/cap with least squares line\"\nggplot(global.data, aes(x=gdp.capita, y=hdi.score)) +\n  geom_point() +\n  labs(x=\"GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\") +\n  geom_smooth(method=\"lm\", se=FALSE)"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#least-squares-line-1",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#least-squares-line-1",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nBut how to calculate?\nMany different ways\n\nMake a line minimizing the least absolute deviations\nNon-parametric lines\nMake a line minimizing the sum of the squares of the deviations\n\nLeast squares line is most common\n\nAdvantages:\n\nEasy to calculate\nWell understood statistical properties\n\nDisadvantages:\n\nLine will be strongly influenced by outliers"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#examining-model-fit",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#examining-model-fit",
    "title": "Simple Linear Regression",
    "section": "Examining model fit",
    "text": "Examining model fit\n\nChecking the residuals\nResidual standard deviation\n\\(R^2\\)\nChecking assumptions\n\n\nChecking the residuals\nAll real datasets have noise so the real formula is:\n\\(y = b_0 + b_1x + e\\)\nResidual = Observed - Predicted\n\n\\(e = y - \\hat{y}\\)\n\nCan easily plot the residuals, put the “size of the miss” on the \\(y\\) axis, and original data on the \\(x\\) axis\n\n\nResiduals - our data\n\n#| label: fig-dataresiduals\n#| caption: \"HDI model with residuals\"\nggplot(global.data.augmented, aes(gdp.capita, hdi.score)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", se=FALSE) +\n  geom_segment(aes(xend = gdp.capita, yend = .fitted), linetype=\"dashed\") + \n  labs(x=\"GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\")"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#graphing-the-residuals",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#graphing-the-residuals",
    "title": "Simple Linear Regression",
    "section": "Graphing the residuals",
    "text": "Graphing the residuals\n\n#| label: fig-residualsplot\n#| caption: \"HDI model residuals plot\"\nggplot(global.data.augmented, aes(gdp.capita, .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0, color = \"blue\", linetype='dashed') + \n  labs(y = \"Residuals\", x=\"GDP per capita (10000 USD) in PPP terms\")\n\n\nResiduals vs. observed data\n\n#| label: fig-residualvsobserved\n#| caption: \"Residuals vs. observed data\"\noriginal &lt;-\n  ggplot(global.data, aes(x=gdp.capita, y=hdi.score)) +\n  geom_point() +\n  labs(x=\"GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\") +\n  geom_smooth(method=\"lm\", se=FALSE)\n\nresids &lt;- ggplot(global.data.augmented, aes(gdp.capita, .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0, color = \"blue\", linetype='dashed') + \n  labs(y = \"Residuals\", x=\"GDP per capita (10000 USD) in PPP terms\")\n\ngrid.arrange(original, resids)"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#residual-standard-deviation",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#residual-standard-deviation",
    "title": "Simple Linear Regression",
    "section": "Residual standard deviation",
    "text": "Residual standard deviation\n\nSince the residuals are just another distribution, we can also examine their distribution\n\nWhat to look for: symmetrical, no skew/outliers\nStandard deviation not too large\n\n\n\nResidual standard deviation - our data\n\n#| label: fig-residualstddev\n#| caption: \"HDI model residuals distribution\"\nggplot(global.data.augmented, aes(x=.resid)) +\n  geom_histogram(fill=\"blue4\") +\n  labs(x=\"Residuals\", y=\"Count\")\n\nHow would you interpret this histogram of the residuals?\n\n#| label: picker3\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#r2",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#r2",
    "title": "Simple Linear Regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\\(R^2\\) is just the return of \\(r\\), the correlation coefficient. Remember:\n\n\\(r\\) measures the strength of the association between \\(x\\) and \\(y\\)\n\nThat is, how reliably \\(x\\) varies with \\(y\\)\n\nThe correlation coefficient: 0.78\nOur \\(R^2\\): 0.61\n\n\n#| label: modelfit\nglobal.data.nooutlier &lt;- global.data %&gt;%\n  filter(gdp.capita &lt; 100000)\n\nhdi.model.nooutlier &lt;- lm(hdi.score ~ gdp.capita, data=global.data.nooutlier)  \nglobal.data.augmented.nooutlier &lt;- augment(hdi.model.nooutlier, global.data.nooutlier)\nhdi.model.tidy.nooutlier &lt;- tidy(hdi.model.nooutlier)\n\nWhat do you think the \\(R^2\\) will change to when we remove the outlier?\n\n#| label: picker4\nsample(classroster$name, 1)\n\n\nThe correlation coefficient for a model with the outlier removed:\n\n\n#| label: correlationnooutlier\nround(cor(global.data.nooutlier$hdi.score, global.data.nooutlier$gdp.capita), digits=2)\n\n\nOur \\(R^2\\) with the outlier removed:\n\n\n#| label: rsquarednooutlier\n\nas.numeric(round(glance(hdi.model.nooutlier)[1], digits=2))"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#how-to-interpret-r2",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#how-to-interpret-r2",
    "title": "Simple Linear Regression",
    "section": "How to interpret \\(R^2\\)",
    "text": "How to interpret \\(R^2\\)\n\nIf there are no serious outliers and the relationship is linear, can provide a useful measure of how strongly the predictor variable is related to the response variable\n\nThe two assumptions above are quite strong - you need to always draw a picture to make sure they are true!\nShould not be interpreted as how strongly \\(x\\) causes \\(y\\), we only know about association."
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#regression-assumptions",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#regression-assumptions",
    "title": "Simple Linear Regression",
    "section": "Regression assumptions",
    "text": "Regression assumptions\n\nQuantitative variable assumption\nStraight enough condition\nOutlier condition\nDoes the plot thicken condition?\n\nHave we met these?\n\n#| label: picker5\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 2.2 - Simple Linear Regression.html#reexpressions",
    "href": "lectures/Lecture 2.2 - Simple Linear Regression.html#reexpressions",
    "title": "Simple Linear Regression",
    "section": "Reexpressions",
    "text": "Reexpressions\n\nLog reexpressed\n\n#| label: logreexpress\nglobal.data.log &lt;- global.data %&gt;%\n  mutate(log.gdp.capita = log(gdp.capita))\n\nhdi.model.log &lt;- lm(hdi.score ~ log.gdp.capita, data=global.data.log)\nglobal.data.augmented.log &lt;- augment(hdi.model.log, global.data.log)\nhdi.model.tidy.log &lt;- tidy(hdi.model.log)\n\nWhat will happen to the shape of the graph?\n\n#| label: picker6\nsample(classroster$name, 1)\n\n\n#| label: fig-logreexpressplot\n#| caption: \"HDI model with log reexpression\"\nggplot(global.data.log, aes(x=log.gdp.capita, y=hdi.score)) +\n  geom_point() +\n  labs(x=\"Log of GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\") +\n  geom_smooth(method=\"lm\", se=FALSE)\n\n\n\nLog reexpressed - outlier\nAny guess as to the outlier?\n\n#| label: picker7\nsample(classroster$name, 1)\n\n\n#| label: fig-modeloutlier\n#| caption: \"HDI model - outlier\"\nggplot(global.data.log, aes(x=log.gdp.capita, y=hdi.score)) +\n  geom_point(aes(color = log.gdp.capita &lt; -6)) +  # color conditionally\n  scale_color_manual(values = c(\"FALSE\" = \"black\", \"TRUE\" = \"red\")) +\n  geom_text(\n    data = subset(global.data.log, log.gdp.capita &lt; -6),  # only label those &lt; -6\n    aes(label = Country),\n    vjust = -0.5,\n    color = \"red\"\n  ) + \n  labs(x=\"Log of GDP per capita (10000 USD) in PPP terms\", y=\"Human Development Index (HDI) score\") \n\n\n\nOutlier\n\n#| label: outliers\n#| caption: \"Outlier information\"\nglobal.data.log %&gt;% \n  filter(log.gdp.capita &lt; -6) %&gt;% \n  mutate(`World Bank PPP GDP estimate` = World.Bank.Forecast.GDP.PPP.*100000,\n         `Population 2023` = Population.2023,\n         `GDP per capita` = gdp.capita*100000) %&gt;% \n  select(c(Country, HDI, `World Bank PPP GDP estimate`, `Population 2023`, `GDP per capita`)) %&gt;% \n  kable()\n\n\n\nGraphing the residuals - log\n\n#| label: fig-residualslog\n#| caption: \"HDI model log/no outlier - residuals\"\nggplot(global.data.augmented.log.nooutlier, aes(log.gdp.capita, .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0, color = \"blue\", linetype='dashed') + \n  labs(y = \"Residuals\", x=\"Log of GDP per capita (10000 USD) in PPP terms\")\n\n\n\nResiduals standard deviation - log\n\n#| label: fig-residualsstddevlog\n#| caption: \"HDI model log/no outlier - residuals distribution\"\nggplot(global.data.augmented.log.nooutlier, aes(x=.resid)) +\n  geom_histogram(fill=\"blue4\") +\n  labs(x=\"Residuals\", y=\"Count\")\n\n\n\n\\(R^2\\)\n\nThe correlation coefficient: 0.95\nOur \\(R^2\\): 0.91\n\n\n\nRegression assumptions\nFor the log reexpressed version, have the assumptions been met?\n\nQuantitative variable assumption\nStraight enough condition\nOutlier condition\nDoes the plot thicken condition?\n\n\n#| label: picker8\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "",
    "text": "#| label: setup\n#| include: false\n#| autorun: true\n\ntheme_set(theme_classic())\n\ntitanic.survival &lt;- read.csv(\"titanic.survival.csv\")\nclassroster &lt;- read.csv(\"classroster.csv\", fileEncoding=\"UTF-8-BOM\")\nnbaboxscores &lt;- read.csv(\"nba2024.2025.csv\")\n\ntitanic.survival &lt;- titanic.survival %&gt;% \n  rename(name = v1)\n\ntitanic_mean &lt;- mean(titanic.survival$age)\ntitanic_sd &lt;- sd(titanic.survival$age)\n\nnbaboxscores &lt;- nbaboxscores %&gt;% \n  mutate(at.home = case_when(home.away == \"home\" ~ TRUE,\n                             TRUE ~ FALSE),\n         div = case_when(team_id == \"BOS\" ~ \"East\",\n                         team_id == \"NYK\" ~ \"East\",\n                         team_id == \"MIL\" ~ \"East\",\n                         team_id == \"CLE\" ~ \"East\",\n                         team_id == \"ORL\" ~ \"East\",\n                         team_id == \"IND\" ~ \"East\",\n                         team_id == \"PHI\" ~ \"East\",\n                         team_id == \"MIA\" ~ \"East\",\n                         team_id == \"CHI\" ~ \"East\",\n                         team_id == \"ATL\" ~ \"East\",\n                         team_id == \"BKN\" ~ \"East\",\n                         team_id == \"TOR\" ~ \"East\",\n                         team_id == \"CHA\" ~ \"East\",\n                         team_id == \"WAS\" ~ \"East\",\n                         team_id == \"DET\" ~ \"East\",\n                         TRUE ~ \"West\"))"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#thoughts-about-comparing-groups",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#thoughts-about-comparing-groups",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Thoughts about comparing groups",
    "text": "Thoughts about comparing groups\n\nFaceted histograms are a reasonable display to show distributions by a categorical variable\n\nHowever these displays become hard to interpret when the number of levels in a category grows large\n\nMuch easier to interpret is side by side box plots\nBox plots capture many important characteristics of a distribution into a summary display\nThink carefully about how you treat outliers\nLet’s view data from the 2024-2025 NBA season"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#two-group-comparison",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#two-group-comparison",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Two group comparison",
    "text": "Two group comparison\n\nNBA side-by-side histograms of points scored by W/L\n\n#| label: nbacomparisonhist\n#| caption: \"Histogram of points scored in NBA games\"\npts.data &lt;- nbaboxscores %&gt;% \n  group_by(winner) %&gt;% \n  mutate(mean.pts = mean(PTS))\n\nggplot(pts.data, aes(x=PTS)) +\n  geom_histogram(fill=\"blue4\") +\n  labs(x=\"Points scored\", y=\"Count\") +\n  facet_wrap(~winner) + \n  geom_vline(aes(xintercept = mean.pts, group=winner), color=\"red\") +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = \"Did the team win?\", breaks = NULL, labels = NULL))\n\n\n\nNBA boxplot comparison of points scored by W/L\n\n#| label: nbacomaprisonbox\n#| caption: \"Boxplot of points scored in NBA games\"\nggplot(nbaboxscores, aes(x=winner,y=PTS)) + \n  geom_boxplot() + \n  labs(x=\"Team won the game?\", y=\"Points scored\")\n\n\n\nNBA boxplot comparison of points scored by W/L (better)\n\n#| label: nbacomparisonbetterbox\n#| caption: \"Beter boxplot of points scored in NBA games\"\nggplot(nbaboxscores, aes(x=winner,y=PTS)) + \n  geom_boxplot(fill=\"light blue\") + \n  labs(x=\"Team won the game?\", y=\"Points scored\") + \n  coord_flip()"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#many-group-comparison",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#many-group-comparison",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Many group comparison",
    "text": "Many group comparison\n\nNBA side-by-side histograms of points scored by team\n\n#| label: nbateamcomparisonhist\n#| caption: \"Histogram of points scored by team\"\nggplot(nbaboxscores, aes(x=PTS)) +\n  geom_histogram(fill=\"blue4\") +\n  labs(x=\"Points scored by team\", y=\"Count\") +\n  facet_wrap( ~ as.factor(team_id))\n\n\n\nNBA boxplot comparison of points scored by team (better)\n\n#| label: nbateamcomparisonbox\n#| caption: \"Boxplot of points scored by team\"\nggplot(nbaboxscores, aes(x=as.factor(team_id),y=PTS, fill=div)) + \n  geom_boxplot() + \n  labs(x=\"Team\", y=\"Points scored by team\") +\n   scale_x_discrete(guide = guide_axis(n.dodge=2)) +\n   guides(fill=guide_legend(title=\"Division\"))\n\n\n\nYour turn\n\nWork with your neighbor to analyze a different set of statistics\n\nCan be by division or not\nRemember the key features of distributions\n\nShape\nCenter\nSpread\n\n\nInterpret your results\nVariable definitions:\n\nMP: minutes played\nFG: field goal baskets made\nFGA: field goal baskets attempted\nFG.: field goal percentage\nX3P: three pointers made\nX3PA: three pointers attempted\nFT: free throws made\nFTA: free throws attempted\nFT.: free throw percentage\nORB: offensive rebounds\nDRB: defensive rebounds\nTRB: total rebounds\nAST: assists\nSTL: steals\nBLK: blocks\nTOV: turnovers\nPF: personal fouls\nPTS: points scored\n\n\n\n#| label: nbateamdescribe\n#| caption: \"Information about the NBA dataset\"\n# Get a list of variable names\nnames(nbaboxscores)\n\n# Take a quick look at the data\nglimpse(nbaboxscores)\n\n\n#| label: picker1\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#checking-outliers---assists",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#checking-outliers---assists",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking outliers - assists",
    "text": "Checking outliers - assists\n\nOutliers - assists\n\n#| label: assistsoutlier\n#| caption: \"Assists outliers\"\nggplot(nbaboxscores, aes(x=AST)) +\n  geom_histogram(fill=\"blue4\", binwidth=1) +\n  labs(x=\"Assists\", y=\"Count\")\n\n\n\nAssists &gt; 40 - true outliers?\n\n#| label: assistsoutliercheck\n#| caption: \"Assists outliers - specific cases\"\nnbaboxscores %&gt;% \n    filter(AST &gt; 40) %&gt;%\n    select(game_id, team_id, home.away, PTS, TRB, gameDate)\n\n\n#| label: picker2\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#checking-outliers---points",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#checking-outliers---points",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking outliers - points",
    "text": "Checking outliers - points\n\nOutliers - points\n\n#| label: pointsoutliers\n#| caption: \"Points outliers\"\nggplot(nbaboxscores, aes(x=PTS)) +\n  geom_histogram(fill=\"blue4\", binwidth=1) +\n  labs(x=\"Points\", y=\"Count\")\n\n\n\nPoints by team &gt; 150 - true outliers?\n\n#| label: pointsoutliercheck\n#| caption: \"Points outliers - specific cases\" \nnbaboxscores %&gt;% \n    filter(PTS &gt; 150) %&gt;%\n    select(game_id, team_id, home.away, PTS, TRB, gameDate)\n\n\n#| label: picker3\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#in-summary",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#in-summary",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "In summary",
    "text": "In summary\n\nThink about which kind of display is appropriate for comparing distributions\nWhen conditioning on a categorical variable, boxplots are usually better\nBut boxplots lose information\nThink carefully about omitting outliers\nOutliers may reveal important information about your dataset!"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#titanic-passengers-and-the-normal-distribution",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#titanic-passengers-and-the-normal-distribution",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Titanic passengers and the Normal distribution",
    "text": "Titanic passengers and the Normal distribution\n\n\n\nTitanic\n\n\n\nDataset of passengers on the Titanic\n\n#| label: datastructure\n#| caption: \"Titanic survival dataset structure\"\nkable(head(titanic.survival))\n\n\nWhat are your expectations for how age should be distributed?\n\n\n#| label: picker4\nsample(classroster$name, 1)\n\n\nWe are going to violate our first three rules:\n\nMake a picture\nMake a picture\nMake a picture\n\n\n\n\nWere the passenger ages normally distributed?\nTo answer that question, we need some information about the distribution\nRemember, our main information about distributions is:\n\nShape\nCenter\nSpread\n\n\n\nInformation about age\n\nStandard deviation: 14.4\nMean: 29.9\nNormal model: \\(N(\\mu, \\sigma) = N(29.9,14.4)\\)\n\n\\(\\mu\\) is the theoretical mean\n\\(\\sigma\\) is the theoretical standard deviation\nThese values define the data generating process\nWe only see some values of the data generating process, but if we saw infinite values, the mean would be \\(\\mu\\) and the sd would be \\(\\sigma\\)\nMore on this in the second half of class\n\nHow can we check normality using this information?\n\n\n#| label: picker5\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#checking-normality",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#checking-normality",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking normality",
    "text": "Checking normality\n\nThinking about normality\n\nWe can check normality by comparing the quantiles of our data with that of the known quantiles of the normal distribution\n\nWe know approximately 95% of the data lies within two standard deviations\nTherefore, 2.5% data with the lowest values lie outside of -2 standard deviations and 2.5% of data with the highest values lie outside of 2 standard deviations\n\nSimilarly, we know the same information for data within one standard deviation (16%, 68%, 16%)\n\n\n\nData within standard deviations\n\n#| label: agesd\n#| caption: \"Standard deviations of age\"\nnorm.dist &lt;- rnorm(1000000)\n\nds.low &lt;- density(norm.dist, from=min(norm.dist), to=-2)\nds.high &lt;- density(norm.dist, from=2, to=max(norm.dist))\nds.low.mid &lt;- density(norm.dist, from=min(norm.dist), to=-1)\nds.high.mid &lt;- density(norm.dist, from=1, to=max(norm.dist))\n\nds.low.data &lt;- data.frame(x = ds.low$x, y = ds.low$y)\nds.low.mid.data &lt;- data.frame(x = ds.low.mid$x, y = ds.low.mid$y)\nds.high.data &lt;- data.frame(x = ds.high$x, y = ds.high$y)\nds.high.mid.data &lt;- data.frame(x = ds.high.mid$x, y = ds.high.mid$y)\n\ntwo.sd &lt;- ggplot(data.frame(norm.dist), aes(norm.dist)) + \n  geom_density() + \n  geom_vline(xintercept=-2, color=\"dark red\") +\n  geom_vline(xintercept=2, color=\"dark red\") +\n  geom_area(data = ds.low.data, aes(x = x, y = y), fill=\"blue4\", color=\"blue4\") +\n  geom_area(data = ds.high.data, aes(x = x, y = y), fill=\"blue4\", color=\"blue4\") + \n  labs(x=\"Standard deviations\", y=\"Density\") + \n  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), limits=(c(-4, 4))) +\n  annotate(geom=\"text\", x=-3, y=0.1, label=\"~2.5% of the data\") +\n  annotate(geom=\"text\", x=3, y=0.1, label=\"~2.5% of the data\") +\n  annotate(geom=\"text\", x=0, y=0.1, label=\"~95% of the data\") \n  \none.sd &lt;- ggplot(data.frame(norm.dist), aes(norm.dist)) +\n  geom_density() +\n  geom_vline(xintercept = -1, color=\"dark red\") +\n  geom_vline(xintercept = 1, color=\"dark red\") +\n  geom_area(data = ds.low.mid.data, aes(x = x, y = y), fill=\"blue4\", color=\"blue4\") +\n  geom_area(data = ds.high.mid.data, aes(x = x, y = y), fill=\"blue4\", color=\"blue4\") +\n  labs(x=\"Standard deviations\", y=\"Density\") + \n  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), limits=(c(-4, 4))) +\n  annotate(geom=\"text\", x=-2.5, y=0.2, label=\"~16% of the data\") +\n  annotate(geom=\"text\", x=2.5, y=0.2, label=\"~16% of the data\") +\n  annotate(geom=\"text\", x=0, y=0.2, label=\"~68% of the data\") \n\ngrid.arrange(two.sd, one.sd)"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#checking-against-the-data",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#checking-against-the-data",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking against the data",
    "text": "Checking against the data\n\nHistogram of ages from the data\n\n#| label: agehist\n#| caption: \"Theoretical vs. actual age distributions\"\nnorm.dist.titanic &lt;- rnorm(1000000, mean=titanic_mean, sd=titanic_sd)\nnorm.dist.titanic &lt;- data.frame(norm.dist.titanic)\n\nggplot(titanic.survival, aes(x=age)) + \n  geom_histogram(aes(y=..density..), fill=\"blue4\") +\n  geom_vline(xintercept=titanic_mean, color=\"dark red\") + \n  geom_density(data=norm.dist.titanic, aes(norm.dist.titanic), color=\"dark red\") +\n  labs(x=\"Age\", y=\"Count\") +\n  scale_x_continuous(limits=c(-10, 80))\n\n\n\nNormality and scaling\n\nNote that normality does not depend on the size of the standard deviation or the size of the mean\nCould easily change the units to be months instead of years\n\nMean would increase a lot\nStandard deviation would increase a lot\nHowever, amount of observations within each standard deviation would stay the same"
  },
  {
    "objectID": "lectures/Lecture 1.3 - Advanced distributions.html#final-thoughts-on-normality",
    "href": "lectures/Lecture 1.3 - Advanced distributions.html#final-thoughts-on-normality",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Final thoughts on normality",
    "text": "Final thoughts on normality\n\nWhen is the normal distribution useful?\n\nWhen we know a data-generating process is normally distributed we don’t even need to sample the population\n\nCan find out exactly how much data is between a certain number of standard deviations\n\nWhen we expect a data-generating process to be normally distributed, can test for deviations from normality\n\nIn the case of Titanic passengers, some parts of the distribution were more bunched up, others more spread out\n\nA lot of our statistical techniques require or work better when the data is ‘roughly’ normal\n\nWill detail these in the coming weeks\n\nWe can transform our data to be closer to normal\n\nNote that transformations won’t work if the data has multiple modes, can only correct skew\n\n\n\n\nWhat transformation would be helpful for age?\n\n#| label: agehist2\n#| caption: \"Transformation check\"\nggplot(titanic.survival, aes(x=age)) + \n  geom_histogram(fill=\"blue4\") +\n  labs(x=\"Age\", y=\"Count\")\n\n\n#| label: picker6\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 1.2 - Characteristics of distributions.html",
    "href": "lectures/Lecture 1.2 - Characteristics of distributions.html",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "",
    "text": "#| label: setup\n#| include: false\n#| autorun: true\n\ntheme_set(theme_classic())\n\nclassroster &lt;- read.csv(\"classroster.csv\", fileEncoding=\"UTF-8-BOM\")\nheights &lt;- read.csv(\"classheights.csv\", fileEncoding=\"UTF-8-BOM\")"
  },
  {
    "objectID": "lectures/Lecture 1.2 - Characteristics of distributions.html#distribution-of-common-quantities",
    "href": "lectures/Lecture 1.2 - Characteristics of distributions.html#distribution-of-common-quantities",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Distribution of common quantities",
    "text": "Distribution of common quantities\nMany phenomena in nature have a relatively easily guessed distribution characteristics\n\nWhat is the distribution of length of rivers in the U.S.?\nWhat is the distribution of width of flower sepals?\nWhat is the distribution of life expectancy across countries in 2007?\n\nFeatures to guess:\n\nShape\nCenter\nSpread\n\n\n#| label: pick1\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 1.2 - Characteristics of distributions.html#graphs-of-common-quantities",
    "href": "lectures/Lecture 1.2 - Characteristics of distributions.html#graphs-of-common-quantities",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Graphs of common quantities",
    "text": "Graphs of common quantities\n\nLength of rivers in the U.S.\n\n#| label: riverlength\n#| caption: \"Graph of river lengths\"\nggplot(data.frame(rivers), aes(x=rivers)) + \n  geom_histogram(fill=\"blue4\", color=\"black\") +\n  labs(x=\"Length in miles\", y=\"count\")\n\n\n\nFlower sepal width\n\n#| label: flowerwidth\n#| caption: \"Graph of flower sepal widths\"\nggplot(iris, aes(x=Sepal.Width)) + \n  geom_histogram(fill=\"blue4\", color=\"black\", bins=15) +\n  labs(x=\"Sepal width\", y=\"Count\")\n\n\n\nLife expetancy in 2007\n\n#| label: lifexpectancy\n#| caption: \"Graph of life expectancies\"\ngap2007 &lt;-gapminder %&gt;%\n  filter(year==2007)\n\nggplot(gap2007, aes(x=lifeExp)) + \n  geom_histogram(fill=\"blue4\", color=\"black\") +\n  labs(x=\"Life expectancy in years\", y=\"Count\")"
  },
  {
    "objectID": "lectures/Lecture 1.2 - Characteristics of distributions.html#height-activity",
    "href": "lectures/Lecture 1.2 - Characteristics of distributions.html#height-activity",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Height activity",
    "text": "Height activity\nWhat do you expect the shape, center, and spread of class height to be? Why? Write down with your partner your guesses.\n\n#| label: pick2\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 1.2 - Characteristics of distributions.html#height-distribution",
    "href": "lectures/Lecture 1.2 - Characteristics of distributions.html#height-distribution",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Height distribution",
    "text": "Height distribution\n\n#| label: heightdistribution\n#| caption: \"Distribution of heights in our class\"\n\nggplot(heights, aes(x=heightincm)) +\n  geom_histogram()"
  },
  {
    "objectID": "lectures/Lecture 1.2 - Characteristics of distributions.html#closing-thoughts",
    "href": "lectures/Lecture 1.2 - Characteristics of distributions.html#closing-thoughts",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Closing thoughts",
    "text": "Closing thoughts\n\nMany distributions can be guessed in advanced based on the data generating process\nYou should have at least a guess as to what the distribution is before starting your exploratory data analysis\nThink carefully about what your variable is actually measuring\nCharacteristics of distributions are summaries of the data, almost always obscure features of the data\nDon’t mislead your readers!!"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html",
    "href": "lectures/Lecture 2.1 - Association and correlation.html",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "",
    "text": "#| label: setup\n#| include: false\n#| autorun: true\n\ntheme_set(theme_classic())\n\nkc.houses &lt;- read.csv(\"king.county.sales.recent.csv\")\nclassroster &lt;- read.csv(\"classroster.csv\", fileEncoding=\"UTF-8-BOM\")\n\nkc.houses &lt;- kc.houses %&gt;% \n  mutate(view = ifelse(rowSums(across(starts_with(\"view\"))) &gt; 0, 1, 0)) %&gt;% \n  mutate(wfnt = factor(wfnt, levels=c(0, 1), labels=c(\"Not on waterfront\", \"On waterfront\"))) %&gt;% \n  mutate(view = factor(view, levels=c(0, 1), labels=c(\"No view\", \"View\")))"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#exercise-1",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#exercise-1",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Exercise 1",
    "text": "Exercise 1\nIn your pairs, try to think of two variables that, in the real world, that might have this correlation for each of the following correlations. Try to think of a few examples for each correlation.\n\n0.95\n0.75\n0.5\n0.25\n0.0\n-0.25\n-0.5\n-0.75\n-0.95\n\nPick a few of these and draw by hand what you expect these graphs to look like.\n\n#| label: pick1\n\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#exercise-2",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#exercise-2",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\nSeattle"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#viewing-an-example-relationship",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#viewing-an-example-relationship",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Viewing an example relationship",
    "text": "Viewing an example relationship\n\nFirst, what is our expectation about the relationship between bedrooms and square feet?\nDirection?\nForm?\nStrength?\nOutliers?"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#bedrooms-and-square-feet---direction",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#bedrooms-and-square-feet---direction",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - direction",
    "text": "Bedrooms and square feet - direction\nA scatterplot is the easiest way to check for direction. In this case, the direction is obvious\n\n#| label: bsqdirection\n\n# should we jitter this?\nggplot(kc.houses, aes(x=sqft, y=beds)) + \n  geom_point() +\n  labs(x=\"Square feet\", y=\"Beds\")"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#bedrooms-and-square-feet---form",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#bedrooms-and-square-feet---form",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - form",
    "text": "Bedrooms and square feet - form\n\n#| label: bsqform\n\nggplot(kc.houses, aes(x=sqft, y=beds)) + \n  geom_point(position=\"jitter\") +\n  geom_smooth(method=\"loess\", se=FALSE) +\n  labs(x=\"Square feet\", y=\"Beds\")"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#bedrooms-and-square-feet---strength",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#bedrooms-and-square-feet---strength",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - strength",
    "text": "Bedrooms and square feet - strength\n\n#| label: bsqstrength\n\nggplot(kc.houses, aes(x=sqft, y=beds)) + \n  geom_point(position=\"jitter\") +\n  geom_smooth(method=\"loess\", se=FALSE) +\n  labs(x=\"Square feet\", y=\"Beds\")\n\n\nCorrelation as a measure of strength\n\n#| label: bsqcorr\n\nkable(kc.houses %&gt;% \n        summarize(cor(beds, sqft, use=\"complete.obs\")), col.names=\"Correlation\")\n\nThis correlation is a little weaker than perhaps what we expected\n\nIn general, mechanically generated processes with little noise can have very high correlations\nMost correlations of social or real world processes rarely have above moderate correlation due to noise"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#bedrooms-and-square-feet---outlier",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#bedrooms-and-square-feet---outlier",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - outlier",
    "text": "Bedrooms and square feet - outlier\nAgain, we do not have a rule for selecting outliers other than to observe them on the scatterplot. In this case, there is one very obvious value far from other values\n\n#| label: bsqoutlierplot\n\nggplot(kc.houses, aes(x=sqft, y=beds)) + \n  geom_point(position=\"jitter\") +\n  geom_smooth(method=\"loess\", se=FALSE)  +\n  geom_text(data=kc.houses %&gt;% filter(sqft &gt; 2500 & beds &lt; 1), # Filter data first\n    aes(label=beds), vjust=0.5, color=\"red\") +\n  labs(x=\"Square feet\", y=\"Beds\")\n\nTo investigate if this outlier matters, we can check some other values of the observation.\n\n#| label: bsqoutlier\n\nkable(\n  kc.houses %&gt;%\n    filter(sqft &gt; 2500 & beds &lt; 1) %&gt;%\n    select(c(sale_price, sqft, sqft_lot, beds, stories)))\n\nWhat kind of outlier do you think this is? Why?\n\nOutlier - actual observation\n\n#| label: bsqoutlier1\n\nkable(\n  kc.houses %&gt;%\n    filter(sqft &gt; 2500 & beds &lt; 1) %&gt;%\n    select(c(latitude, longitude)))\n\n\n\n\nLocation\n\n\n\n\n\nHouse details\n\n\n\n\nData with no outlier\n\n#| label: bsqnooutlier\n#| message: false\n\n#should we jitter this?\nkc.houses %&gt;%\n  filter(sqft &lt; 2500 | beds &gt; 1) %&gt;%  \n  ggplot(aes(x=sqft, y=beds)) + \n    geom_point(position=\"jitter\") +\n    geom_smooth(method=\"loess\", se=FALSE) +\n    labs(x=\"Square feet\", y=\"Beds\")\n\n\n\nDescribing the association\n\nDirection - positive\nForm - linear\nStrength - moderate/strong\nOutliers - one possible\n\nOutlier:\n\n#| label:  bsqoutlier2\n\nkable(\n  kc.houses %&gt;%\n    filter(sqft&gt;2500 & beds &lt; 1) %&gt;%\n    select(c(sale_price, sqft, sqft_lot, latitude, longitude)))"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#does-bed-and-square-feeet-relationship-match-expectations",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#does-bed-and-square-feeet-relationship-match-expectations",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Does bed and square feeet relationship match expectations?",
    "text": "Does bed and square feeet relationship match expectations?\n\nSeems to, more or less, the larger the house, the more bedrooms, so the relationship is positive\nThe relationship is fairly linear, indicating a strong relationship\nRelationship is moderate\nOutliers don’t seem like it is a problem\n\nHowever….\n\nWhat are some possible lurking variables that influence the relationship of bedrooms and bathrooms?\n\n\n#| label: pick2\n\nsample(classroster$name, 1)"
  },
  {
    "objectID": "lectures/Lecture 2.1 - Association and correlation.html#relationship-reexpressed",
    "href": "lectures/Lecture 2.1 - Association and correlation.html#relationship-reexpressed",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Relationship reexpressed?",
    "text": "Relationship reexpressed?\nOne final issue to consider is if this relationship should be reexpressed - made more linear.\n\n#| label: bsqreexpress\n\nkc.houses %&gt;%\n  mutate(log.sqft = log(sqft)) %&gt;%\n  ggplot(aes(x=log.sqft, y=beds)) + \n    geom_point() +\n    geom_smooth(method=\"lm\", se=FALSE) +\n    labs(x=\"Log square feet\", y=\"Beds\")\n\n\nSeems clearer\n\n\nYour turn\nWith your partner, develop some expectations about some of the variables in the kc.houses dataset might be related.\nVariables:\n\n#| label: datasetdetails\n\nkc.houses.short &lt;- kc.houses %&gt;% \n  select(c(sale_price, year_built, sqft, sqft_lot, beds, bath_full, view, stories))\n\nkable(names(kc.houses.short), col.names=c(\"Variables\"))\n\nWhat to do with your partner:\n\nWrite down an interesting question we think we might be able to answer by examining a relationship in this dataset\nChoose the variables that you think might be able to answer this question\nWrite down what you expect the relationship to be between these two variables based on any prior knowledge\nDecide which variable is the response variable and which is the predictor variable\nMake a scatterplot using one of the codeblocks in the previous section and identify the features of the association\n\n\n#| label: pick3\nsample(classroster$name, 1)"
  }
]